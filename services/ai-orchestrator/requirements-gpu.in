######## GPU / Extended ML Dependency Layer ########
# This file layers GPU-heavy dependencies on top of the lean CPU baseline in requirements.in
# Regenerate GPU lock with:
#   pip-compile --generate-hashes --allow-unsafe \
#       --output-file services/ai-orchestrator/constraints-gpu.txt \
#       services/ai-orchestrator/requirements-gpu.in
# Policy: Keep base environment lightweight (no torch / CUDA libs) for faster cold starts and smaller images.
# Only use this profile in GPU-enabled deployments or for advanced model fine-tuning flows.

-r requirements.in

######## Core GPU Framework ########
# Single authoritative PyTorch pin. Keep only one torch version to avoid resolver conflicts.
torch==2.8.0

# Transformer / Accelerator stack relocated from base to keep CPU layer slim.
######## Transformer Stack (moved from base) ########
transformers==4.44.2
accelerate==0.33.0
safetensors==0.4.5

# Future optional GPU-specific acceleration libraries (add explicitly, never implicit):
# xformers==<pin>
# flash-attn==<pin>
# GPU / heavy ML optional extras
# Install with: pip install -r requirements-gpu.in -c constraints-gpu.txt

# Optional ecosystem (add only when required for deployment/runtime)
# torchvision==<match_torch_version>
# torchaudio==<match_torch_version>
# deepSpeed==0.15.4
# bitsandbytes==0.43.1

# If moving accelerate out of base, pin here instead (kept in base presently for CPU workflows)
# accelerate==0.33.0
