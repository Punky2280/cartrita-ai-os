# Chat Completions API

> **Source**: https://platform.openai.com/docs/api-reference/chat
> **Last Updated**: September 17, 2025

## Overview

Given a list of messages comprising a conversation, the model will return a response. The Chat Completions API is the primary interface for interacting with OpenAI's language models.

## Create Chat Completion

Creates a model response for the given chat conversation.

### HTTP Request
```
POST https://api.openai.com/v1/chat/completions
```

### Request Body

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `model` | string | Yes | ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API. |
| `messages` | array | Yes | A list of messages comprising the conversation so far. |
| `max_tokens` | integer | No | The maximum number of tokens that can be generated in the chat completion. |
| `temperature` | number | No | What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. |
| `top_p` | number | No | An alternative to sampling with temperature, called nucleus sampling. |
| `n` | integer | No | How many chat completion choices to generate for each input message. |
| `stream` | boolean | No | If set, partial message deltas will be sent as data-only server-sent events. |
| `stop` | string or array | No | Up to 4 sequences where the API will stop generating further tokens. |
| `presence_penalty` | number | No | Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far. |
| `frequency_penalty` | number | No | Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far. |
| `logit_bias` | map | No | Modify the likelihood of specified tokens appearing in the completion. |
| `user` | string | No | A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. |
| `functions` | array | No | A list of functions the model may generate JSON inputs for. |
| `function_call` | string or object | No | Controls how the model responds to function calls. |
| `tools` | array | No | A list of tools the model may call. Currently, only functions are supported as a tool. |
| `tool_choice` | string or object | No | Controls which (if any) tool is called by the model. |

### Message Object

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `role` | string | Yes | The role of the message author. One of `system`, `user`, `assistant`, or `function`. |
| `content` | string | Yes | The content of the message. |
| `name` | string | No | The name of the author of this message. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters. |
| `function_call` | object | No | The name and arguments of a function that should be called, as generated by the model. |
| `tool_calls` | array | No | The tool calls generated by the model, such as function calls. |

### Example Request

#### Basic Chat Completion
```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ],
    max_tokens=100,
    temperature=0.7
)

print(response.choices[0].message.content)
```

#### Streaming Response
```python
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "Count to 10"}
    ],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```

#### Function Calling
```python
from openai import OpenAI
import json

client = OpenAI()

# Define a function
functions = [
    {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city and state, e.g. San Francisco, CA",
                },
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
            },
            "required": ["location"],
        },
    }
]

messages = [{"role": "user", "content": "What's the weather like in Boston?"}]

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=messages,
    functions=functions,
    function_call="auto"
)

response_message = response.choices[0].message

# Check if the model wants to call a function
if response_message.function_call:
    # Call the function
    function_name = response_message.function_call.name
    function_args = json.loads(response_message.function_call.arguments)

    # Here you would call your actual function
    function_response = get_current_weather(
        location=function_args.get("location"),
        unit=function_args.get("unit", "fahrenheit")
    )

    # Add the function response to the conversation
    messages.append(response_message)
    messages.append({
        "role": "function",
        "name": function_name,
        "content": function_response,
    })

    # Get the final response
    second_response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
    )

    print(second_response.choices[0].message.content)
```

### Response Object

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "gpt-3.5-turbo-0613",
  "system_fingerprint": "fp_44709d6fcb",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello there, how may I assist you today?"
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 56,
    "completion_tokens": 31,
    "total_tokens": 87
  }
}
```

### Choice Object

| Parameter | Type | Description |
|-----------|------|-------------|
| `index` | integer | The index of the choice in the list of choices. |
| `message` | object | A chat completion message generated by the model. |
| `logprobs` | object | Log probability information for the choice. |
| `finish_reason` | string | The reason the model stopped generating tokens. |

### Finish Reasons

| Reason | Description |
|--------|-------------|
| `stop` | Model hit a natural stopping point or provided stop sequence. |
| `length` | Model hit the token limit specified by max_tokens. |
| `function_call` | Model decided to call a function. |
| `content_filter` | Content omitted due to a flag from content filters. |
| `null` | API response still in progress or incomplete. |

### Usage Object

| Parameter | Type | Description |
|-----------|------|-------------|
| `prompt_tokens` | integer | Number of tokens in the prompt. |
| `completion_tokens` | integer | Number of tokens in the generated completion. |
| `total_tokens` | integer | Total number of tokens used in the request. |

## Streaming

When `stream=true`, the response will be a stream of chunk objects. Each chunk follows this format:

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion.chunk",
  "created": 1677652288,
  "model": "gpt-3.5-turbo-0613",
  "system_fingerprint": "fp_44709d6fcb",
  "choices": [
    {
      "index": 0,
      "delta": {
        "content": "Hello"
      },
      "logprobs": null,
      "finish_reason": null
    }
  ]
}
```

### Handling Streaming Responses

```python
def handle_stream(stream):
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            content = chunk.choices[0].delta.content
            print(content, end="", flush=True)
```

## Function Calling

The Chat Completions API supports function calling, allowing you to provide function descriptions to the model and receive structured data back.

### Function Schema

```json
{
  "name": "function_name",
  "description": "Description of what the function does",
  "parameters": {
    "type": "object",
    "properties": {
      "parameter_name": {
        "type": "string",
        "description": "Description of the parameter"
      }
    },
    "required": ["parameter_name"]
  }
}
```

### Tool Calling (New Format)

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get weather information",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The location to get weather for"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "What's the weather in NYC?"}],
    tools=tools,
    tool_choice="auto"
)
```

## Error Handling

### Common Error Codes

| Code | Description |
|------|-------------|
| 400 | Bad Request - Invalid request format |
| 401 | Unauthorized - Invalid API key |
| 403 | Forbidden - Request forbidden |
| 404 | Not Found - Resource not found |
| 429 | Too Many Requests - Rate limit exceeded |
| 500 | Internal Server Error - Server error |
| 503 | Service Unavailable - Server overloaded |

### Error Response Format

```json
{
  "error": {
    "message": "Invalid request: messages must be an array",
    "type": "invalid_request_error",
    "param": "messages",
    "code": null
  }
}
```

### Python Error Handling

```python
import openai

try:
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "Hello!"}]
    )
except openai.RateLimitError as e:
    print(f"Rate limit exceeded: {e}")
except openai.APIError as e:
    print(f"API error: {e}")
except Exception as e:
    print(f"Unexpected error: {e}")
```

## Best Practices

### Prompt Engineering
- Use clear and specific instructions
- Provide examples when possible
- Use system messages to set context
- Break complex tasks into steps

### Performance Optimization
- Use appropriate models for your use case
- Implement caching for repeated queries
- Use streaming for real-time applications
- Monitor token usage to control costs

### Security
- Validate and sanitize user inputs
- Implement content filtering
- Use the user parameter for abuse monitoring
- Never expose API keys in client-side code

### Cost Management
- Monitor token usage regularly
- Use max_tokens to limit response length
- Choose cost-effective models when possible
- Implement usage limits and budgets

---

*This documentation covers the Chat Completions API. For more information about other OpenAI APIs, see the complete API reference.*
