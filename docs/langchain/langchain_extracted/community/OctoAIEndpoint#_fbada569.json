{
  "url": "https://python.langchain.com/api_reference/community/llms/langchain_community.llms.octoai_endpoint.OctoAIEndpoint.html",
  "title": "OctoAIEndpoint#",
  "sections": [
    {
      "type": "li",
      "content": "LangChain Python API Reference"
    },
    {
      "type": "li",
      "content": "langchain-community: 0.3.29"
    },
    {
      "type": "li",
      "content": "OctoAIEndpoint"
    },
    {
      "type": "p",
      "content": "Bases:BaseOpenAI"
    },
    {
      "type": "p",
      "content": "OctoAI LLM Endpoints - OpenAI compatible."
    },
    {
      "type": "p",
      "content": "OctoAIEndpoint is a class to interact with OctoAI Compute Service large\nlanguage model endpoints."
    },
    {
      "type": "p",
      "content": "To use, you should have the environment variableOCTOAI_API_TOKENset\nwith your API token, or pass it as a named parameter to the constructor."
    },
    {
      "type": "p",
      "content": "Initialize the OpenAI object."
    },
    {
      "type": "p",
      "content": "OctoAIEndpoint implements the standardRunnableInterface. üèÉ"
    },
    {
      "type": "p",
      "content": "TheRunnableInterfacehas additional methods that are available on runnables, such aswith_config,with_types,with_retry,assign,bind,get_graph, and more."
    },
    {
      "type": "p",
      "content": "Set of special tokens that are allowed„ÄÇ"
    },
    {
      "type": "p",
      "content": "Batch size to use when passing multiple documents to generate."
    },
    {
      "type": "p",
      "content": "Generates best_of completions server-side and returns the ‚Äúbest‚Äù."
    },
    {
      "type": "p",
      "content": "Whether to cache the response."
    },
    {
      "type": "li",
      "content": "If true, will use the global cache."
    },
    {
      "type": "p",
      "content": "If true, will use the global cache."
    },
    {
      "type": "li",
      "content": "If false, will not use a cache"
    },
    {
      "type": "p",
      "content": "If false, will not use a cache"
    },
    {
      "type": "li",
      "content": "If None, will use the global cache if it‚Äôs set, otherwise no cache."
    },
    {
      "type": "p",
      "content": "If None, will use the global cache if it‚Äôs set, otherwise no cache."
    },
    {
      "type": "li",
      "content": "If instance ofBaseCache, will use the provided cache."
    },
    {
      "type": "p",
      "content": "If instance ofBaseCache, will use the provided cache."
    },
    {
      "type": "p",
      "content": "Caching is not currently supported for streaming methods of models."
    },
    {
      "type": "p",
      "content": "[DEPRECATED]"
    },
    {
      "type": "p",
      "content": "Callbacks to add to the run trace."
    },
    {
      "type": "p",
      "content": "Optional encoder to use for counting tokens."
    },
    {
      "type": "p",
      "content": "Set of special tokens that are not allowed„ÄÇ"
    },
    {
      "type": "p",
      "content": "Penalizes repeated tokens according to frequency."
    },
    {
      "type": "p",
      "content": "Optional httpx.Client."
    },
    {
      "type": "p",
      "content": "Adjust the probability of specific tokens being generated."
    },
    {
      "type": "p",
      "content": "Maximum number of retries to make when generating."
    },
    {
      "type": "p",
      "content": "The maximum number of tokens to generate in the completion.\n-1 returns as many tokens as possible given the prompt and\nthe models maximal context size."
    },
    {
      "type": "p",
      "content": "Metadata to add to the run trace."
    },
    {
      "type": "p",
      "content": "Holds any model parameters valid forcreatecall not explicitly specified."
    },
    {
      "type": "p",
      "content": "Model name to use."
    },
    {
      "type": "p",
      "content": "How many completions to generate for each prompt."
    },
    {
      "type": "p",
      "content": "Base URL path for API requests, leave blank if not using a proxy or service\nemulator."
    },
    {
      "type": "p",
      "content": "Automatically inferred from env varOPENAI_API_KEYif not provided."
    },
    {
      "type": "p",
      "content": "Automatically inferred from env varOPENAI_ORG_IDif not provided."
    },
    {
      "type": "p",
      "content": "Penalizes repeated tokens."
    },
    {
      "type": "p",
      "content": "Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or\nNone."
    },
    {
      "type": "p",
      "content": "Whether to stream the results or not."
    },
    {
      "type": "p",
      "content": "Tags to add to the run trace."
    },
    {
      "type": "p",
      "content": "What sampling temperature to use."
    },
    {
      "type": "p",
      "content": "The model name to pass to tiktoken when using this class.\nTiktoken is used to count the number of tokens in documents to constrain\nthem to be under a certain limit. By default, when set to None, this will\nbe the same as the embedding model name. However, there are some cases\nwhere you may want to use this Embedding class with a model name not\nsupported by tiktoken. This can include when using Azure embeddings or\nwhen using one of the many model providers that expose an OpenAI-like\nAPI but with different models. In those cases, in order to avoid erroring\nwhen tiktoken is called, you can specify a model name to use here."
    },
    {
      "type": "p",
      "content": "Total probability mass of tokens to consider at each step."
    },
    {
      "type": "p",
      "content": "Whether to print out response text."
    },
    {
      "type": "p",
      "content": "Validate that api key and python package exists in environment."
    },
    {
      "type": "p",
      "content": "values(Dict)"
    },
    {
      "type": "p",
      "content": "Calculate the maximum number of tokens possible to generate for a model."
    },
    {
      "type": "p",
      "content": "modelname(str) ‚Äì The modelname we want to know the context size for."
    },
    {
      "type": "p",
      "content": "The maximum context size"
    },
    {
      "type": "p",
      "content": "Deprecated since version 0.1.7:Useinvoke()instead. It will not be removed until langchain-core==1.0."
    },
    {
      "type": "p",
      "content": "Check Cache and run the LLM on the given prompt and input."
    },
    {
      "type": "li",
      "content": "prompt(str) ‚Äì The prompt to generate from."
    },
    {
      "type": "p",
      "content": "prompt(str) ‚Äì The prompt to generate from."
    },
    {
      "type": "li",
      "content": "stop(list[str]|None) ‚Äì Stop words to use when generating. Model output is cut off at the\nfirst occurrence of any of these substrings."
    },
    {
      "type": "p",
      "content": "stop(list[str]|None) ‚Äì Stop words to use when generating. Model output is cut off at the\nfirst occurrence of any of these substrings."
    },
    {
      "type": "li",
      "content": "callbacks(list[BaseCallbackHandler]|BaseCallbackManager|None) ‚Äì Callbacks to pass through. Used for executing additional\nfunctionality, such as logging or streaming, throughout generation."
    },
    {
      "type": "p",
      "content": "callbacks(list[BaseCallbackHandler]|BaseCallbackManager|None) ‚Äì Callbacks to pass through. Used for executing additional\nfunctionality, such as logging or streaming, throughout generation."
    },
    {
      "type": "li",
      "content": "tags(list[str]|None) ‚Äì List of tags to associate with the prompt."
    },
    {
      "type": "p",
      "content": "tags(list[str]|None) ‚Äì List of tags to associate with the prompt."
    },
    {
      "type": "li",
      "content": "metadata(dict[str,Any]|None) ‚Äì Metadata to associate with the prompt."
    },
    {
      "type": "p",
      "content": "metadata(dict[str,Any]|None) ‚Äì Metadata to associate with the prompt."
    },
    {
      "type": "li",
      "content": "**kwargs(Any) ‚Äì Arbitrary additional keyword arguments. These are usually passed\nto the model provider API call."
    },
    {
      "type": "p",
      "content": "**kwargs(Any) ‚Äì Arbitrary additional keyword arguments. These are usually passed\nto the model provider API call."
    },
    {
      "type": "p",
      "content": "The generated text."
    },
    {
      "type": "p",
      "content": "ValueError‚Äì If the prompt is not a string."
    },
    {
      "type": "p",
      "content": "Default implementation runsainvokein parallel usingasyncio.gather."
    },
    {
      "type": "p",
      "content": "The default implementation ofbatchworks well for IO bound runnables."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they can batch more efficiently;\ne.g., if the underlyingRunnableuses an API which supports a batch mode."
    },
    {
      "type": "li",
      "content": "inputs(list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "**kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A list of outputs from theRunnable."
    },
    {
      "type": "p",
      "content": "Runainvokein parallel on a list of inputs."
    },
    {
      "type": "p",
      "content": "Yields results as they complete."
    },
    {
      "type": "li",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A tuple of the index of the input and the output from theRunnable."
    },
    {
      "type": "p",
      "content": "AsyncIterator[tuple[int,Output| Exception]]"
    },
    {
      "type": "p",
      "content": "Transform a single input into an output."
    },
    {
      "type": "li",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "li",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "Default implementation ofastream, which callsainvoke."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they support streaming output."
    },
    {
      "type": "li",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|None) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|None) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "li",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "AsyncIterator[str]"
    },
    {
      "type": "p",
      "content": "Generate a stream of events."
    },
    {
      "type": "p",
      "content": "Use to create an iterator overStreamEventsthat provide real-time information\nabout the progress of theRunnable, includingStreamEventsfrom intermediate\nresults."
    },
    {
      "type": "p",
      "content": "AStreamEventis a dictionary with the following schema:"
    },
    {
      "type": "li",
      "content": "event:str- Event names are of the format:on_[runnable_type]_(start|stream|end)."
    },
    {
      "type": "p",
      "content": "event:str- Event names are of the format:on_[runnable_type]_(start|stream|end)."
    },
    {
      "type": "li",
      "content": "name:str- The name of theRunnablethat generated the event."
    },
    {
      "type": "p",
      "content": "name:str- The name of theRunnablethat generated the event."
    },
    {
      "type": "li",
      "content": "run_id:str- randomly generated ID associated with the given\nexecution of theRunnablethat emitted the event. A childRunnablethat gets\ninvoked as part of the execution of a parentRunnableis assigned its own\nunique ID."
    },
    {
      "type": "p",
      "content": "run_id:str- randomly generated ID associated with the given\nexecution of theRunnablethat emitted the event. A childRunnablethat gets\ninvoked as part of the execution of a parentRunnableis assigned its own\nunique ID."
    },
    {
      "type": "li",
      "content": "parent_ids:list[str]- The IDs of the parent runnables that generated\nthe event. The rootRunnablewill have an empty list. The order of the parent\nIDs is from the root to the immediate parent. Only available for v2 version of\nthe API. The v1 version of the API will return an empty list."
    },
    {
      "type": "p",
      "content": "parent_ids:list[str]- The IDs of the parent runnables that generated\nthe event. The rootRunnablewill have an empty list. The order of the parent\nIDs is from the root to the immediate parent. Only available for v2 version of\nthe API. The v1 version of the API will return an empty list."
    },
    {
      "type": "li",
      "content": "tags:Optional[list[str]]- The tags of theRunnablethat generated\nthe event."
    },
    {
      "type": "p",
      "content": "tags:Optional[list[str]]- The tags of theRunnablethat generated\nthe event."
    },
    {
      "type": "li",
      "content": "metadata:Optional[dict[str, Any]]- The metadata of theRunnablethat\ngenerated the event."
    },
    {
      "type": "p",
      "content": "metadata:Optional[dict[str, Any]]- The metadata of theRunnablethat\ngenerated the event."
    },
    {
      "type": "li",
      "content": "data:dict[str, Any]"
    },
    {
      "type": "p",
      "content": "data:dict[str, Any]"
    },
    {
      "type": "p",
      "content": "Below is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table."
    },
    {
      "type": "p",
      "content": "This reference table is for the v2 version of the schema."
    },
    {
      "type": "p",
      "content": "on_chat_model_start"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "{\"messages\":[[SystemMessage,HumanMessage]]}"
    },
    {
      "type": "p",
      "content": "on_chat_model_stream"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "AIMessageChunk(content=\"hello\")"
    },
    {
      "type": "p",
      "content": "on_chat_model_end"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "{\"messages\":[[SystemMessage,HumanMessage]]}"
    },
    {
      "type": "p",
      "content": "AIMessageChunk(content=\"helloworld\")"
    },
    {
      "type": "p",
      "content": "on_llm_start"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "{'input':'hello'}"
    },
    {
      "type": "p",
      "content": "on_llm_stream"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "``‚ÄôHello‚Äô ``"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "'Hellohuman!'"
    },
    {
      "type": "p",
      "content": "on_chain_start"
    },
    {
      "type": "p",
      "content": "format_docs"
    },
    {
      "type": "p",
      "content": "on_chain_stream"
    },
    {
      "type": "p",
      "content": "format_docs"
    },
    {
      "type": "p",
      "content": "'helloworld!,goodbyeworld!'"
    },
    {
      "type": "p",
      "content": "on_chain_end"
    },
    {
      "type": "p",
      "content": "format_docs"
    },
    {
      "type": "p",
      "content": "[Document(...)]"
    },
    {
      "type": "p",
      "content": "'helloworld!,goodbyeworld!'"
    },
    {
      "type": "p",
      "content": "on_tool_start"
    },
    {
      "type": "p",
      "content": "{\"x\":1,\"y\":\"2\"}"
    },
    {
      "type": "p",
      "content": "on_tool_end"
    },
    {
      "type": "p",
      "content": "{\"x\":1,\"y\":\"2\"}"
    },
    {
      "type": "p",
      "content": "on_retriever_start"
    },
    {
      "type": "p",
      "content": "[retriever name]"
    },
    {
      "type": "p",
      "content": "{\"query\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "on_retriever_end"
    },
    {
      "type": "p",
      "content": "[retriever name]"
    },
    {
      "type": "p",
      "content": "{\"query\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "[Document(...),..]"
    },
    {
      "type": "p",
      "content": "on_prompt_start"
    },
    {
      "type": "p",
      "content": "[template_name]"
    },
    {
      "type": "p",
      "content": "{\"question\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "on_prompt_end"
    },
    {
      "type": "p",
      "content": "[template_name]"
    },
    {
      "type": "p",
      "content": "{\"question\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "ChatPromptValue(messages:[SystemMessage,...])"
    },
    {
      "type": "p",
      "content": "In addition to the standard events, users can also dispatch custom events (see example below)."
    },
    {
      "type": "p",
      "content": "Custom events will be only be surfaced with in the v2 version of the API!"
    },
    {
      "type": "p",
      "content": "A custom event has following format:"
    },
    {
      "type": "p",
      "content": "Description"
    },
    {
      "type": "p",
      "content": "A user defined name for the event."
    },
    {
      "type": "p",
      "content": "The data associated with the event. This can be anything, though we suggest making it JSON serializable."
    },
    {
      "type": "p",
      "content": "Here are declarations associated with the standard events shown above:"
    },
    {
      "type": "p",
      "content": "format_docs:"
    },
    {
      "type": "p",
      "content": "Example: Dispatch Custom Event"
    },
    {
      "type": "li",
      "content": "input(Any) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(Any) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(Optional[RunnableConfig]) ‚Äì The config to use for theRunnable."
    },
    {
      "type": "p",
      "content": "config(Optional[RunnableConfig]) ‚Äì The config to use for theRunnable."
    },
    {
      "type": "li",
      "content": "version(Literal['v1','v2']) ‚Äì The version of the schema to use either'v2'or'v1'.\nUsers should use'v2'.'v1'is for backwards compatibility and will be deprecated\nin 0.4.0.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in'v2'."
    },
    {
      "type": "p",
      "content": "version(Literal['v1','v2']) ‚Äì The version of the schema to use either'v2'or'v1'.\nUsers should use'v2'.'v1'is for backwards compatibility and will be deprecated\nin 0.4.0.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in'v2'."
    },
    {
      "type": "li",
      "content": "include_names(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching names."
    },
    {
      "type": "p",
      "content": "include_names(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching names."
    },
    {
      "type": "li",
      "content": "include_types(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching types."
    },
    {
      "type": "p",
      "content": "include_types(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching types."
    },
    {
      "type": "li",
      "content": "include_tags(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching tags."
    },
    {
      "type": "p",
      "content": "include_tags(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching tags."
    },
    {
      "type": "li",
      "content": "exclude_names(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching names."
    },
    {
      "type": "p",
      "content": "exclude_names(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching names."
    },
    {
      "type": "li",
      "content": "exclude_types(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching types."
    },
    {
      "type": "p",
      "content": "exclude_types(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching types."
    },
    {
      "type": "li",
      "content": "exclude_tags(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching tags."
    },
    {
      "type": "p",
      "content": "exclude_tags(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching tags."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable.\nThese will be passed toastream_logas this implementation\nofastream_eventsis built on top ofastream_log."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable.\nThese will be passed toastream_logas this implementation\nofastream_eventsis built on top ofastream_log."
    },
    {
      "type": "p",
      "content": "An async stream ofStreamEvents."
    },
    {
      "type": "p",
      "content": "NotImplementedError‚Äì If the version is not'v1'or'v2'."
    },
    {
      "type": "p",
      "content": "AsyncIterator[StreamEvent]"
    },
    {
      "type": "p",
      "content": "Default implementation runs invoke in parallel using a thread pool executor."
    },
    {
      "type": "p",
      "content": "The default implementation of batch works well for IO bound runnables."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they can batch more efficiently;\ne.g., if the underlyingRunnableuses an API which supports a batch mode."
    },
    {
      "type": "li",
      "content": "inputs(list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work\nto do in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work\nto do in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "**kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A list of outputs from theRunnable."
    },
    {
      "type": "p",
      "content": "Runinvokein parallel on a list of inputs."
    },
    {
      "type": "p",
      "content": "Yields results as they complete."
    },
    {
      "type": "li",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "**kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "Tuples of the index of the input and the output from theRunnable."
    },
    {
      "type": "p",
      "content": "Iterator[tuple[int,Output| Exception]]"
    },
    {
      "type": "p",
      "content": "Bind arguments to aRunnable, returning a newRunnable."
    },
    {
      "type": "p",
      "content": "Useful when aRunnablein a chain requires an argument that is not\nin the output of the previousRunnableor included in the user input."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì The arguments to bind to theRunnable."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the arguments bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input,Output]"
    },
    {
      "type": "p",
      "content": "Configure alternatives forRunnablesthat can be set at runtime."
    },
    {
      "type": "li",
      "content": "which(ConfigurableField) ‚Äì TheConfigurableFieldinstance that will be used to select the\nalternative."
    },
    {
      "type": "p",
      "content": "which(ConfigurableField) ‚Äì TheConfigurableFieldinstance that will be used to select the\nalternative."
    },
    {
      "type": "li",
      "content": "default_key(str) ‚Äì The default key to use if no alternative is selected.\nDefaults to'default'."
    },
    {
      "type": "p",
      "content": "default_key(str) ‚Äì The default key to use if no alternative is selected.\nDefaults to'default'."
    },
    {
      "type": "li",
      "content": "prefix_keys(bool) ‚Äì Whether to prefix the keys with theConfigurableFieldid.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "prefix_keys(bool) ‚Äì Whether to prefix the keys with theConfigurableFieldid.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]) ‚Äì A dictionary of keys toRunnableinstances or callables that\nreturnRunnableinstances."
    },
    {
      "type": "p",
      "content": "**kwargs(Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]) ‚Äì A dictionary of keys toRunnableinstances or callables that\nreturnRunnableinstances."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the alternatives configured."
    },
    {
      "type": "p",
      "content": "RunnableSerializable"
    },
    {
      "type": "p",
      "content": "Configure particularRunnablefields at runtime."
    },
    {
      "type": "p",
      "content": "**kwargs(ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption) ‚Äì A dictionary ofConfigurableFieldinstances to configure."
    },
    {
      "type": "p",
      "content": "ValueError‚Äì If a configuration key is not found in theRunnable."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the fields configured."
    },
    {
      "type": "p",
      "content": "RunnableSerializable"
    },
    {
      "type": "p",
      "content": "Create the LLMResult from the choices and prompts."
    },
    {
      "type": "li",
      "content": "choices(Any)"
    },
    {
      "type": "p",
      "content": "choices(Any)"
    },
    {
      "type": "li",
      "content": "prompts(List[str])"
    },
    {
      "type": "p",
      "content": "prompts(List[str])"
    },
    {
      "type": "li",
      "content": "params(Dict[str,Any])"
    },
    {
      "type": "p",
      "content": "params(Dict[str,Any])"
    },
    {
      "type": "li",
      "content": "token_usage(Dict[str,int])"
    },
    {
      "type": "p",
      "content": "token_usage(Dict[str,int])"
    },
    {
      "type": "li",
      "content": "system_fingerprint(str|None)"
    },
    {
      "type": "p",
      "content": "system_fingerprint(str|None)"
    },
    {
      "type": "p",
      "content": "Get the number of tokens present in the text."
    },
    {
      "type": "p",
      "content": "Useful for checking if an input fits in a model‚Äôs context window."
    },
    {
      "type": "p",
      "content": "text(str) ‚Äì The string input to tokenize."
    },
    {
      "type": "p",
      "content": "The integer number of tokens in the text."
    },
    {
      "type": "p",
      "content": "Get the number of tokens in the messages."
    },
    {
      "type": "p",
      "content": "Useful for checking if an input fits in a model‚Äôs context window."
    },
    {
      "type": "p",
      "content": "The base implementation ofget_num_tokens_from_messagesignores tool\nschemas."
    },
    {
      "type": "li",
      "content": "messages(list[BaseMessage]) ‚Äì The message inputs to tokenize."
    },
    {
      "type": "p",
      "content": "messages(list[BaseMessage]) ‚Äì The message inputs to tokenize."
    },
    {
      "type": "li",
      "content": "tools(Sequence|None) ‚Äì If provided, sequence of dict,BaseModel, function, orBaseToolsto be converted to tool schemas."
    },
    {
      "type": "p",
      "content": "tools(Sequence|None) ‚Äì If provided, sequence of dict,BaseModel, function, orBaseToolsto be converted to tool schemas."
    },
    {
      "type": "p",
      "content": "The sum of the number of tokens across the messages."
    },
    {
      "type": "p",
      "content": "Get the sub prompts for llm call."
    },
    {
      "type": "li",
      "content": "params(Dict[str,Any])"
    },
    {
      "type": "p",
      "content": "params(Dict[str,Any])"
    },
    {
      "type": "li",
      "content": "prompts(List[str])"
    },
    {
      "type": "p",
      "content": "prompts(List[str])"
    },
    {
      "type": "li",
      "content": "stop(List[str]|None)"
    },
    {
      "type": "p",
      "content": "stop(List[str]|None)"
    },
    {
      "type": "p",
      "content": "List[List[str]]"
    },
    {
      "type": "p",
      "content": "Get the token IDs using the tiktoken package."
    },
    {
      "type": "p",
      "content": "Transform a single input into an output."
    },
    {
      "type": "li",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "li",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "Calculate the maximum number of tokens possible to generate for a prompt."
    },
    {
      "type": "p",
      "content": "prompt(str) ‚Äì The prompt to pass into the model."
    },
    {
      "type": "p",
      "content": "The maximum number of tokens to generate for a prompt."
    },
    {
      "type": "p",
      "content": "Save the LLM."
    },
    {
      "type": "p",
      "content": "file_path(Path|str) ‚Äì Path to file to save the LLM to."
    },
    {
      "type": "p",
      "content": "ValueError‚Äì If the file path is not a string or Path object."
    },
    {
      "type": "p",
      "content": "Default implementation ofstream, which callsinvoke."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they support streaming output."
    },
    {
      "type": "li",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|None) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|None) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "li",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "Iterator[str]"
    },
    {
      "type": "p",
      "content": "Bind async lifecycle listeners to aRunnable."
    },
    {
      "type": "p",
      "content": "Returns a newRunnable."
    },
    {
      "type": "p",
      "content": "The Run object contains information about the run, including itsid,type,input,output,error,start_time,end_time, and\nany tags or metadata added to the run."
    },
    {
      "type": "li",
      "content": "on_start(Optional[AsyncListener]) ‚Äì Called asynchronously before theRunnablestarts running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_start(Optional[AsyncListener]) ‚Äì Called asynchronously before theRunnablestarts running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_end(Optional[AsyncListener]) ‚Äì Called asynchronously after theRunnablefinishes running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_end(Optional[AsyncListener]) ‚Äì Called asynchronously after theRunnablefinishes running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_error(Optional[AsyncListener]) ‚Äì Called asynchronously if theRunnablethrows an error,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_error(Optional[AsyncListener]) ‚Äì Called asynchronously if theRunnablethrows an error,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the listeners bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input, Output]"
    },
    {
      "type": "p",
      "content": "Bind config to aRunnable, returning a newRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|None) ‚Äì The config to bind to theRunnable."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|None) ‚Äì The config to bind to theRunnable."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the config bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input,Output]"
    },
    {
      "type": "p",
      "content": "Add fallbacks to aRunnable, returning a newRunnable."
    },
    {
      "type": "p",
      "content": "The newRunnablewill try the originalRunnable, and then each fallback\nin order, upon failures."
    },
    {
      "type": "li",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "p",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "li",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle.\nDefaults to(Exception,)."
    },
    {
      "type": "p",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle.\nDefaults to(Exception,)."
    },
    {
      "type": "li",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input. Defaults to None."
    },
    {
      "type": "p",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input. Defaults to None."
    },
    {
      "type": "p",
      "content": "A newRunnablethat will try the originalRunnable, and then each\nfallback in order, upon failures."
    },
    {
      "type": "p",
      "content": "RunnableWithFallbacksT[Input, Output]"
    },
    {
      "type": "li",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "p",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "li",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle."
    },
    {
      "type": "p",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle."
    },
    {
      "type": "li",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input."
    },
    {
      "type": "p",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input."
    },
    {
      "type": "p",
      "content": "A newRunnablethat will try the originalRunnable, and then each\nfallback in order, upon failures."
    },
    {
      "type": "p",
      "content": "RunnableWithFallbacksT[Input, Output]"
    },
    {
      "type": "p",
      "content": "Bind lifecycle listeners to aRunnable, returning a newRunnable."
    },
    {
      "type": "p",
      "content": "The Run object contains information about the run, including itsid,type,input,output,error,start_time,end_time, and\nany tags or metadata added to the run."
    },
    {
      "type": "li",
      "content": "on_start(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called before theRunnablestarts running, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_start(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called before theRunnablestarts running, with theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_end(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called after theRunnablefinishes running, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_end(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called after theRunnablefinishes running, with theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_error(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called if theRunnablethrows an error, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_error(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called if theRunnablethrows an error, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the listeners bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input, Output]"
    },
    {
      "type": "p",
      "content": "Create a new Runnable that retries the original Runnable on exceptions."
    },
    {
      "type": "li",
      "content": "retry_if_exception_type(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to retry on.\nDefaults to (Exception,)."
    },
    {
      "type": "p",
      "content": "retry_if_exception_type(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to retry on.\nDefaults to (Exception,)."
    },
    {
      "type": "li",
      "content": "wait_exponential_jitter(bool) ‚Äì Whether to add jitter to the wait\ntime between retries. Defaults to True."
    },
    {
      "type": "p",
      "content": "wait_exponential_jitter(bool) ‚Äì Whether to add jitter to the wait\ntime between retries. Defaults to True."
    },
    {
      "type": "li",
      "content": "stop_after_attempt(int) ‚Äì The maximum number of attempts to make before\ngiving up. Defaults to 3."
    },
    {
      "type": "p",
      "content": "stop_after_attempt(int) ‚Äì The maximum number of attempts to make before\ngiving up. Defaults to 3."
    },
    {
      "type": "li",
      "content": "exponential_jitter_params(Optional[ExponentialJitterParams]) ‚Äì Parameters fortenacity.wait_exponential_jitter. Namely:initial,max,exp_base, andjitter(all float values)."
    },
    {
      "type": "p",
      "content": "exponential_jitter_params(Optional[ExponentialJitterParams]) ‚Äì Parameters fortenacity.wait_exponential_jitter. Namely:initial,max,exp_base, andjitter(all float values)."
    },
    {
      "type": "p",
      "content": "A new Runnable that retries the original Runnable on exceptions."
    },
    {
      "type": "p",
      "content": "Runnable[Input, Output]"
    },
    {
      "type": "p",
      "content": "Not implemented on this class."
    },
    {
      "type": "li",
      "content": "schema(dict|type)"
    },
    {
      "type": "p",
      "content": "schema(dict|type)"
    },
    {
      "type": "li",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "Runnable[PromptValue| str |Sequence[BaseMessage| list[str] | tuple[str, str] | str | dict[str,Any]], dict |BaseModel]"
    },
    {
      "type": "p",
      "content": "Bind input and output types to aRunnable, returning a newRunnable."
    },
    {
      "type": "li",
      "content": "input_type(type[Input]|None) ‚Äì The input type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "input_type(type[Input]|None) ‚Äì The input type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "li",
      "content": "output_type(type[Output]|None) ‚Äì The output type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "output_type(type[Output]|None) ‚Äì The output type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "A new Runnable with the types bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input,Output]"
    },
    {
      "type": "p",
      "content": "Get max context size for this model."
    },
    {
      "type": "p",
      "content": "Examples using OctoAIEndpoint"
    },
    {
      "type": "li",
      "content": "OctoAIEndpointallowed_specialbatch_sizebest_ofcachecallback_managercallbackscustom_get_token_idsdefault_headersdefault_querydisallowed_specialfrequency_penaltyhttp_clientlogit_biasmax_retriesmax_tokensmetadatamodel_kwargsmodel_namenoctoai_api_baseoctoai_api_tokenopenai_api_baseopenai_api_keyopenai_organizationopenai_proxypresence_penaltyrequest_timeoutstreamingtagstemperaturetiktoken_model_nametop_pverbosevalidate_environment()modelname_to_contextsize()__call__()abatch()abatch_as_completed()ainvoke()astream()astream_events()batch()batch_as_completed()bind()configurable_alternatives()configurable_fields()create_llm_result()get_num_tokens()get_num_tokens_from_messages()get_sub_prompts()get_token_ids()invoke()max_tokens_for_prompt()save()stream()with_alisteners()with_config()with_fallbacks()with_listeners()with_retry()with_structured_output()with_types()max_context_size"
    },
    {
      "type": "li",
      "content": "allowed_special"
    },
    {
      "type": "li",
      "content": "callback_manager"
    },
    {
      "type": "li",
      "content": "custom_get_token_ids"
    },
    {
      "type": "li",
      "content": "default_headers"
    },
    {
      "type": "li",
      "content": "default_query"
    },
    {
      "type": "li",
      "content": "disallowed_special"
    },
    {
      "type": "li",
      "content": "frequency_penalty"
    },
    {
      "type": "li",
      "content": "http_client"
    },
    {
      "type": "li",
      "content": "max_retries"
    },
    {
      "type": "li",
      "content": "model_kwargs"
    },
    {
      "type": "li",
      "content": "octoai_api_base"
    },
    {
      "type": "li",
      "content": "octoai_api_token"
    },
    {
      "type": "li",
      "content": "openai_api_base"
    },
    {
      "type": "li",
      "content": "openai_api_key"
    },
    {
      "type": "li",
      "content": "openai_organization"
    },
    {
      "type": "li",
      "content": "openai_proxy"
    },
    {
      "type": "li",
      "content": "presence_penalty"
    },
    {
      "type": "li",
      "content": "request_timeout"
    },
    {
      "type": "li",
      "content": "temperature"
    },
    {
      "type": "li",
      "content": "tiktoken_model_name"
    },
    {
      "type": "li",
      "content": "validate_environment()"
    },
    {
      "type": "li",
      "content": "modelname_to_contextsize()"
    },
    {
      "type": "li",
      "content": "abatch_as_completed()"
    },
    {
      "type": "li",
      "content": "astream_events()"
    },
    {
      "type": "li",
      "content": "batch_as_completed()"
    },
    {
      "type": "li",
      "content": "configurable_alternatives()"
    },
    {
      "type": "li",
      "content": "configurable_fields()"
    },
    {
      "type": "li",
      "content": "create_llm_result()"
    },
    {
      "type": "li",
      "content": "get_num_tokens()"
    },
    {
      "type": "li",
      "content": "get_num_tokens_from_messages()"
    },
    {
      "type": "li",
      "content": "get_sub_prompts()"
    },
    {
      "type": "li",
      "content": "get_token_ids()"
    },
    {
      "type": "li",
      "content": "max_tokens_for_prompt()"
    },
    {
      "type": "li",
      "content": "with_alisteners()"
    },
    {
      "type": "li",
      "content": "with_config()"
    },
    {
      "type": "li",
      "content": "with_fallbacks()"
    },
    {
      "type": "li",
      "content": "with_listeners()"
    },
    {
      "type": "li",
      "content": "with_retry()"
    },
    {
      "type": "li",
      "content": "with_structured_output()"
    },
    {
      "type": "li",
      "content": "with_types()"
    },
    {
      "type": "li",
      "content": "max_context_size"
    }
  ],
  "code_examples": [
    "llms",
    "BaseOpenAI",
    "OCTOAI_API_TOKEN",
    "fromlangchain_community.llms.octoai_endpointimportOctoAIEndpointllm=OctoAIEndpoint(model=\"llama-2-13b-chat-fp16\",max_tokens=200,presence_penalty=0,temperature=0.1,top_p=0.9,)",
    "RunnableInterface",
    "RunnableInterface",
    "with_config",
    "with_types",
    "with_retry",
    "assign",
    "bind",
    "get_graph",
    "BaseCache",
    "max_tokens=openai.modelname_to_contextsize(\"gpt-3.5-turbo-instruct\")",
    "invoke()",
    "ainvoke",
    "asyncio.gather",
    "batch",
    "Runnable",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "ainvoke",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "astream",
    "ainvoke",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "StreamEvents",
    "Runnable",
    "StreamEvents",
    "StreamEvent",
    "event",
    "on_[runnable_type]_(start|stream|end)",
    "name",
    "Runnable",
    "run_id",
    "Runnable",
    "Runnable",
    "Runnable",
    "parent_ids",
    "Runnable",
    "tags",
    "Runnable",
    "metadata",
    "Runnable",
    "data",
    "on_chat_model_start",
    "{\"messages\":[[SystemMessage,HumanMessage]]}",
    "on_chat_model_stream",
    "AIMessageChunk(content=\"hello\")",
    "on_chat_model_end",
    "{\"messages\":[[SystemMessage,HumanMessage]]}",
    "AIMessageChunk(content=\"helloworld\")",
    "on_llm_start",
    "{'input':'hello'}",
    "on_llm_stream",
    "on_llm_end",
    "'Hellohuman!'",
    "on_chain_start",
    "on_chain_stream",
    "'helloworld!,goodbyeworld!'",
    "on_chain_end",
    "[Document(...)]",
    "'helloworld!,goodbyeworld!'",
    "on_tool_start",
    "{\"x\":1,\"y\":\"2\"}",
    "on_tool_end",
    "{\"x\":1,\"y\":\"2\"}",
    "on_retriever_start",
    "{\"query\":\"hello\"}",
    "on_retriever_end",
    "{\"query\":\"hello\"}",
    "[Document(...),..]",
    "on_prompt_start",
    "{\"question\":\"hello\"}",
    "on_prompt_end",
    "{\"question\":\"hello\"}",
    "ChatPromptValue(messages:[SystemMessage,...])",
    "format_docs",
    "defformat_docs(docs:list[Document])->str:'''Format the docs.'''return\", \".join([doc.page_contentfordocindocs])format_docs=RunnableLambda(format_docs)",
    "some_tool",
    "@tooldefsome_tool(x:int,y:str)->dict:'''Some_tool.'''return{\"x\":x,\"y\":y}",
    "prompt",
    "template=ChatPromptTemplate.from_messages([(\"system\",\"You are Cat Agent 007\"),(\"human\",\"{question}\")]).with_config({\"run_name\":\"my_template\",\"tags\":[\"my_template\"]})",
    "fromlangchain_core.runnablesimportRunnableLambdaasyncdefreverse(s:str)->str:returns[::-1]chain=RunnableLambda(func=reverse)events=[eventasyncforeventinchain.astream_events(\"hello\",version=\"v2\")]# will produce the following events (run_id, and parent_ids# has been omitted for brevity):[{\"data\":{\"input\":\"hello\"},\"event\":\"on_chain_start\",\"metadata\":{},\"name\":\"reverse\",\"tags\":[],},{\"data\":{\"chunk\":\"olleh\"},\"event\":\"on_chain_stream\",\"metadata\":{},\"name\":\"reverse\",\"tags\":[],},{\"data\":{\"output\":\"olleh\"},\"event\":\"on_chain_end\",\"metadata\":{},\"name\":\"reverse\",\"tags\":[],},]",
    "fromlangchain_core.callbacks.managerimport(adispatch_custom_event,)fromlangchain_core.runnablesimportRunnableLambda,RunnableConfigimportasyncioasyncdefslow_thing(some_input:str,config:RunnableConfig)->str:\"\"\"Do something that takes a long time.\"\"\"awaitasyncio.sleep(1)# Placeholder for some slow operationawaitadispatch_custom_event(\"progress_event\",{\"message\":\"Finished step 1 of 3\"},config=config# Must be included for python < 3.10)awaitasyncio.sleep(1)# Placeholder for some slow operationawaitadispatch_custom_event(\"progress_event\",{\"message\":\"Finished step 2 of 3\"},config=config# Must be included for python < 3.10)awaitasyncio.sleep(1)# Placeholder for some slow operationreturn\"Done\"slow_thing=RunnableLambda(slow_thing)asyncforeventinslow_thing.astream_events(\"some_input\",version=\"v2\"):print(event)",
    "Runnable",
    "Runnable",
    "'v2'",
    "'v1'",
    "'v2'",
    "'v1'",
    "'v2'",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnable",
    "astream_log",
    "astream_events",
    "astream_log",
    "StreamEvents",
    "'v1'",
    "'v2'",
    "Runnable",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "invoke",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "fromlangchain_ollamaimportChatOllamafromlangchain_core.output_parsersimportStrOutputParserllm=ChatOllama(model=\"llama2\")# Without bind.chain=llm|StrOutputParser()chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")# Output is 'One two three four five.'# With bind.chain=llm.bind(stop=[\"three\"])|StrOutputParser()chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")# Output is 'One two'",
    "Runnables",
    "ConfigurableField",
    "'default'",
    "ConfigurableField",
    "Runnable",
    "Runnable",
    "Runnable",
    "fromlangchain_anthropicimportChatAnthropicfromlangchain_core.runnables.utilsimportConfigurableFieldfromlangchain_openaiimportChatOpenAImodel=ChatAnthropic(model_name=\"claude-3-7-sonnet-20250219\").configurable_alternatives(ConfigurableField(id=\"llm\"),default_key=\"anthropic\",openai=ChatOpenAI(),)# uses the default model ChatAnthropicprint(model.invoke(\"which organization created you?\").content)# uses ChatOpenAIprint(model.with_config(configurable={\"llm\":\"openai\"}).invoke(\"which organization created you?\").content)",
    "Runnable",
    "ConfigurableField",
    "Runnable",
    "Runnable",
    "fromlangchain_core.runnablesimportConfigurableFieldfromlangchain_openaiimportChatOpenAImodel=ChatOpenAI(max_tokens=20).configurable_fields(max_tokens=ConfigurableField(id=\"output_token_number\",name=\"Max tokens in the output\",description=\"The maximum number of tokens in the output\",))# max_tokens = 20print(\"max_tokens_20: \",model.invoke(\"tell me something about chess\").content)# max_tokens = 200print(\"max_tokens_200: \",model.with_config(configurable={\"output_token_number\":200}).invoke(\"tell me something about chess\").content,)",
    "get_num_tokens_from_messages",
    "BaseModel",
    "BaseTools",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "max_tokens=openai.max_tokens_for_prompt(\"Tell me a joke.\")",
    "llm.save(file_path=\"path/llm.yaml\")",
    "stream",
    "invoke",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "id",
    "type",
    "input",
    "output",
    "error",
    "start_time",
    "end_time",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "fromlangchain_core.runnablesimportRunnableLambda,Runnablefromdatetimeimportdatetime,timezoneimporttimeimportasynciodefformat_t(timestamp:float)->str:returndatetime.fromtimestamp(timestamp,tz=timezone.utc).isoformat()asyncdeftest_runnable(time_to_sleep:int):print(f\"Runnable[{time_to_sleep}s]: starts at{format_t(time.time())}\")awaitasyncio.sleep(time_to_sleep)print(f\"Runnable[{time_to_sleep}s]: ends at{format_t(time.time())}\")asyncdeffn_start(run_obj:Runnable):print(f\"on start callback starts at{format_t(time.time())}\")awaitasyncio.sleep(3)print(f\"on start callback ends at{format_t(time.time())}\")asyncdeffn_end(run_obj:Runnable):print(f\"on end callback starts at{format_t(time.time())}\")awaitasyncio.sleep(2)print(f\"on end callback ends at{format_t(time.time())}\")runnable=RunnableLambda(test_runnable).with_alisteners(on_start=fn_start,on_end=fn_end)asyncdefconcurrent_runs():awaitasyncio.gather(runnable.ainvoke(2),runnable.ainvoke(3))asyncio.run(concurrent_runs())Result:onstartcallbackstartsat2025-03-01T07:05:22.875378+00:00onstartcallbackstartsat2025-03-01T07:05:22.875495+00:00onstartcallbackendsat2025-03-01T07:05:25.878862+00:00onstartcallbackendsat2025-03-01T07:05:25.878947+00:00Runnable[2s]:startsat2025-03-01T07:05:25.879392+00:00Runnable[3s]:startsat2025-03-01T07:05:25.879804+00:00Runnable[2s]:endsat2025-03-01T07:05:27.881998+00:00onendcallbackstartsat2025-03-01T07:05:27.882360+00:00Runnable[3s]:endsat2025-03-01T07:05:28.881737+00:00onendcallbackstartsat2025-03-01T07:05:28.882428+00:00onendcallbackendsat2025-03-01T07:05:29.883893+00:00onendcallbackendsat2025-03-01T07:05:30.884831+00:00",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "(Exception,)",
    "Runnable",
    "Runnable",
    "Runnable",
    "fromtypingimportIteratorfromlangchain_core.runnablesimportRunnableGeneratordef_generate_immediate_error(input:Iterator)->Iterator[str]:raiseValueError()yield\"\"def_generate(input:Iterator)->Iterator[str]:yield from\"foo bar\"runnable=RunnableGenerator(_generate_immediate_error).with_fallbacks([RunnableGenerator(_generate)])print(\"\".join(runnable.stream({})))# foo bar",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "id",
    "type",
    "input",
    "output",
    "error",
    "start_time",
    "end_time",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "fromlangchain_core.runnablesimportRunnableLambdafromlangchain_core.tracers.schemasimportRunimporttimedeftest_runnable(time_to_sleep:int):time.sleep(time_to_sleep)deffn_start(run_obj:Run):print(\"start_time:\",run_obj.start_time)deffn_end(run_obj:Run):print(\"end_time:\",run_obj.end_time)chain=RunnableLambda(test_runnable).with_listeners(on_start=fn_start,on_end=fn_end)chain.invoke(2)",
    "tenacity.wait_exponential_jitter",
    "initial",
    "max",
    "exp_base",
    "jitter",
    "fromlangchain_core.runnablesimportRunnableLambdacount=0def_lambda(x:int)->None:globalcountcount=count+1ifx==1:raiseValueError(\"x is 1\")else:passrunnable=RunnableLambda(_lambda)try:runnable.with_retry(stop_after_attempt=2,retry_if_exception_type=(ValueError,),).invoke(1)exceptValueError:passassertcount==2",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "OctoAIEndpoint",
    "allowed_special",
    "batch_size",
    "best_of",
    "cache",
    "callback_manager",
    "callbacks",
    "custom_get_token_ids",
    "default_headers",
    "default_query",
    "disallowed_special",
    "frequency_penalty",
    "http_client",
    "logit_bias",
    "max_retries",
    "max_tokens",
    "metadata",
    "model_kwargs",
    "model_name",
    "n",
    "octoai_api_base",
    "octoai_api_token",
    "openai_api_base",
    "openai_api_key",
    "openai_organization",
    "openai_proxy",
    "presence_penalty",
    "request_timeout",
    "streaming",
    "tags",
    "temperature",
    "tiktoken_model_name",
    "top_p",
    "verbose",
    "validate_environment()",
    "modelname_to_contextsize()",
    "__call__()",
    "abatch()",
    "abatch_as_completed()",
    "ainvoke()",
    "astream()",
    "astream_events()",
    "batch()",
    "batch_as_completed()",
    "bind()",
    "configurable_alternatives()",
    "configurable_fields()",
    "create_llm_result()",
    "get_num_tokens()",
    "get_num_tokens_from_messages()",
    "get_sub_prompts()",
    "get_token_ids()",
    "invoke()",
    "max_tokens_for_prompt()",
    "save()",
    "stream()",
    "with_alisteners()",
    "with_config()",
    "with_fallbacks()",
    "with_listeners()",
    "with_retry()",
    "with_structured_output()",
    "with_types()",
    "max_context_size"
  ],
  "api_signatures": [
    "classlangchain_community.llms.octoai_endpoint.OctoAIEndpoint[source]#",
    "langchain_community.llms.octoai_endpoint.",
    "OctoAIEndpoint",
    "paramallowed_special:Literal['all']|AbstractSet[str]={}#",
    "allowed_special",
    "parambatch_size:int=20#",
    "batch_size",
    "parambest_of:int=1#",
    "best_of",
    "paramcache:BaseCache|bool|None=None#",
    "cache",
    "paramcallback_manager:BaseCallbackManager|None=None#",
    "callback_manager",
    "paramcallbacks:Callbacks=None#",
    "callbacks",
    "paramcustom_get_token_ids:Callable[[str],list[int]]|None=None#",
    "custom_get_token_ids",
    "paramdefault_headers:Mapping[str,str]|None=None#",
    "default_headers",
    "paramdefault_query:Mapping[str,object]|None=None#",
    "default_query",
    "paramdisallowed_special:Literal['all']|Collection[str]='all'#",
    "disallowed_special",
    "paramfrequency_penalty:float=0#",
    "frequency_penalty",
    "paramhttp_client:Any|None=None#",
    "http_client",
    "paramlogit_bias:Dict[str,float]|None[Optional]#",
    "logit_bias",
    "parammax_retries:int=2#",
    "max_retries",
    "parammax_tokens:int=256#",
    "max_tokens",
    "parammetadata:dict[str,Any]|None=None#",
    "metadata",
    "parammodel_kwargs:Dict[str,Any][Optional]#",
    "model_kwargs",
    "parammodel_name:str='codellama-7b-instruct'#",
    "model_name",
    "paramn:int=1#",
    "n",
    "paramoctoai_api_base:str='https://text.octoai.run/v1/'#",
    "octoai_api_base",
    "paramoctoai_api_token:SecretStr=SecretStr('')#",
    "octoai_api_token",
    "paramopenai_api_base:str|None=None(alias'base_url')#",
    "openai_api_base",
    "paramopenai_api_key:str|None=None(alias'api_key')#",
    "openai_api_key",
    "paramopenai_organization:str|None=None(alias'organization')#",
    "openai_organization",
    "paramopenai_proxy:str|None=None#",
    "openai_proxy",
    "parampresence_penalty:float=0#",
    "presence_penalty",
    "paramrequest_timeout:float|Tuple[float,float]|Any|None=None(alias'timeout')#",
    "request_timeout",
    "paramstreaming:bool=False#",
    "streaming",
    "paramtags:list[str]|None=None#",
    "tags",
    "paramtemperature:float=0.7#",
    "temperature",
    "paramtiktoken_model_name:str|None=None#",
    "tiktoken_model_name",
    "paramtop_p:float=1#",
    "top_p",
    "paramverbose:bool[Optional]#",
    "verbose",
    "classmethodvalidate_environment(values:Dict,)‚ÜíDict[source]#",
    "validate_environment",
    "(",
    "values:Dict",
    ")",
    "‚ÜíDict",
    "‚Üí",
    "Dict",
    "staticmodelname_to_contextsize(modelname:str,)‚Üíint#",
    "modelname_to_contextsize",
    "(",
    "modelname:str",
    ")",
    "‚Üíint",
    "‚Üí",
    "int",
    "__call__(prompt:str,stop:list[str]|None=None,callbacks:list[BaseCallbackHandler]|BaseCallbackManager|None=None,*,tags:list[str]|None=None,metadata:dict[str,Any]|None=None,**kwargs:Any,)‚Üístr#",
    "__call__",
    "(",
    "prompt:str",
    "stop:list[str]|None=None",
    "callbacks:list[BaseCallbackHandler]|BaseCallbackManager|None=None",
    "*",
    "tags:list[str]|None=None",
    "metadata:dict[str,Any]|None=None",
    "**kwargs:Any",
    ")",
    "‚Üístr",
    "‚Üí",
    "str",
    "asyncabatch(inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]],config:RunnableConfig|list[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any,)‚Üílist[str]#",
    "abatch",
    "(",
    "inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any",
    ")",
    "‚Üílist[str]",
    "‚Üí",
    "list[str]",
    "asyncabatch_as_completed(inputs:Sequence[Input],config:RunnableConfig|Sequence[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any|None,)‚ÜíAsyncIterator[tuple[int,Output|Exception]]#",
    "abatch_as_completed",
    "(",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    ")",
    "‚ÜíAsyncIterator[tuple[int,Output|Exception]]",
    "‚Üí",
    "AsyncIterator[tuple[int,Output|Exception]]",
    "asyncainvoke(input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚Üístr#",
    "ainvoke",
    "(",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚Üístr",
    "‚Üí",
    "str",
    "asyncastream(input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚ÜíAsyncIterator[str]#",
    "astream",
    "(",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíAsyncIterator[str]",
    "‚Üí",
    "AsyncIterator[str]",
    "asyncastream_events(input:Any,config:RunnableConfig|None=None,*,version:Literal['v1','v2']='v2',include_names:Sequence[str]|None=None,include_types:Sequence[str]|None=None,include_tags:Sequence[str]|None=None,exclude_names:Sequence[str]|None=None,exclude_types:Sequence[str]|None=None,exclude_tags:Sequence[str]|None=None,**kwargs:Any,)‚ÜíAsyncIterator[StreamEvent]#",
    "astream_events",
    "(",
    "input:Any",
    "config:RunnableConfig|None=None",
    "*",
    "version:Literal['v1','v2']='v2'",
    "include_names:Sequence[str]|None=None",
    "include_types:Sequence[str]|None=None",
    "include_tags:Sequence[str]|None=None",
    "exclude_names:Sequence[str]|None=None",
    "exclude_types:Sequence[str]|None=None",
    "exclude_tags:Sequence[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíAsyncIterator[StreamEvent]",
    "‚Üí",
    "AsyncIterator[StreamEvent]",
    "batch(inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]],config:RunnableConfig|list[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any,)‚Üílist[str]#",
    "batch",
    "(",
    "inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any",
    ")",
    "‚Üílist[str]",
    "‚Üí",
    "list[str]",
    "batch_as_completed(inputs:Sequence[Input],config:RunnableConfig|Sequence[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any|None,)‚ÜíIterator[tuple[int,Output|Exception]]#",
    "batch_as_completed",
    "(",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    ")",
    "‚ÜíIterator[tuple[int,Output|Exception]]",
    "‚Üí",
    "Iterator[tuple[int,Output|Exception]]",
    "bind(**kwargs:Any,)‚ÜíRunnable[Input,Output]#",
    "bind",
    "(",
    "**kwargs:Any",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "configurable_alternatives(which:ConfigurableField,*,default_key:str='default',prefix_keys:bool=False,**kwargs:Runnable[Input,Output]|Callable[[],Runnable[Input,Output]],)‚ÜíRunnableSerializable#",
    "configurable_alternatives",
    "(",
    "which:ConfigurableField",
    "*",
    "default_key:str='default'",
    "prefix_keys:bool=False",
    "**kwargs:Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]",
    ")",
    "‚ÜíRunnableSerializable",
    "‚Üí",
    "RunnableSerializable",
    "configurable_fields(**kwargs:ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption,)‚ÜíRunnableSerializable#",
    "configurable_fields",
    "(",
    "**kwargs:ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption",
    ")",
    "‚ÜíRunnableSerializable",
    "‚Üí",
    "RunnableSerializable",
    "create_llm_result(choices:Any,prompts:List[str],params:Dict[str,Any],token_usage:Dict[str,int],*,system_fingerprint:str|None=None,)‚ÜíLLMResult#",
    "create_llm_result",
    "(",
    "choices:Any",
    "prompts:List[str]",
    "params:Dict[str,Any]",
    "token_usage:Dict[str,int]",
    "*",
    "system_fingerprint:str|None=None",
    ")",
    "‚ÜíLLMResult",
    "‚Üí",
    "LLMResult",
    "get_num_tokens(text:str)‚Üíint#",
    "get_num_tokens",
    "(",
    "text:str",
    ")",
    "‚Üíint",
    "‚Üí",
    "int",
    "get_num_tokens_from_messages(messages:list[BaseMessage],tools:Sequence|None=None,)‚Üíint#",
    "get_num_tokens_from_messages",
    "(",
    "messages:list[BaseMessage]",
    "tools:Sequence|None=None",
    ")",
    "‚Üíint",
    "‚Üí",
    "int",
    "get_sub_prompts(params:Dict[str,Any],prompts:List[str],stop:List[str]|None=None,)‚ÜíList[List[str]]#",
    "get_sub_prompts",
    "(",
    "params:Dict[str,Any]",
    "prompts:List[str]",
    "stop:List[str]|None=None",
    ")",
    "‚ÜíList[List[str]]",
    "‚Üí",
    "List[List[str]]",
    "get_token_ids(text:str)‚ÜíList[int]#",
    "get_token_ids",
    "(",
    "text:str",
    ")",
    "‚ÜíList[int]",
    "‚Üí",
    "List[int]",
    "invoke(input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚Üístr#",
    "invoke",
    "(",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚Üístr",
    "‚Üí",
    "str",
    "max_tokens_for_prompt(prompt:str)‚Üíint#",
    "max_tokens_for_prompt",
    "(",
    "prompt:str",
    ")",
    "‚Üíint",
    "‚Üí",
    "int",
    "save(file_path:Path|str)‚ÜíNone#",
    "save",
    "(",
    "file_path:Path|str",
    ")",
    "‚ÜíNone",
    "‚Üí",
    "None",
    "stream(input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚ÜíIterator[str]#",
    "stream",
    "(",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíIterator[str]",
    "‚Üí",
    "Iterator[str]",
    "with_alisteners(*,on_start:AsyncListener|None=None,on_end:AsyncListener|None=None,on_error:AsyncListener|None=None,)‚ÜíRunnable[Input,Output]#",
    "with_alisteners",
    "(",
    "*",
    "on_start:AsyncListener|None=None",
    "on_end:AsyncListener|None=None",
    "on_error:AsyncListener|None=None",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_config(config:RunnableConfig|None=None,**kwargs:Any,)‚ÜíRunnable[Input,Output]#",
    "with_config",
    "(",
    "config:RunnableConfig|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_fallbacks(fallbacks:Sequence[Runnable[Input,Output]],*,exceptions_to_handle:tuple[type[BaseException],...]=(<class'Exception'>,),exception_key:Optional[str]=None)‚ÜíRunnableWithFallbacksT[Input,Output]#",
    "with_fallbacks",
    "(",
    "fallbacks:Sequence[Runnable[Input,Output]],*,exceptions_to_handle:tuple[type[BaseException],...]=(<class'Exception'>,),exception_key:Optional[str]=None",
    ")",
    "‚ÜíRunnableWithFallbacksT[Input,Output]",
    "‚Üí",
    "RunnableWithFallbacksT[Input,Output]",
    "with_listeners(*,on_start:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None,on_end:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None,on_error:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None,)‚ÜíRunnable[Input,Output]#",
    "with_listeners",
    "(",
    "*",
    "on_start:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_end:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_error:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_retry(*,retry_if_exception_type:tuple[type[BaseException],...]=(<class'Exception'>,),wait_exponential_jitter:bool=True,exponential_jitter_params:Optional[ExponentialJitterParams]=None,stop_after_attempt:int=3)‚ÜíRunnable[Input,Output]#",
    "with_retry",
    "(",
    "*,retry_if_exception_type:tuple[type[BaseException],...]=(<class'Exception'>,),wait_exponential_jitter:bool=True,exponential_jitter_params:Optional[ExponentialJitterParams]=None,stop_after_attempt:int=3",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_structured_output(schema:dict|type,**kwargs:Any,)‚ÜíRunnable[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],dict|BaseModel]#",
    "with_structured_output",
    "(",
    "schema:dict|type",
    "**kwargs:Any",
    ")",
    "‚ÜíRunnable[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],dict|BaseModel]",
    "‚Üí",
    "Runnable[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],dict|BaseModel]",
    "with_types(*,input_type:type[Input]|None=None,output_type:type[Output]|None=None,)‚ÜíRunnable[Input,Output]#",
    "with_types",
    "(",
    "*",
    "input_type:type[Input]|None=None",
    "output_type:type[Output]|None=None",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "propertymax_context_size:int#",
    "max_context_size"
  ],
  "parameters": [
    "values:Dict",
    "modelname:str",
    "prompt:str",
    "stop:list[str]|None=None",
    "callbacks:list[BaseCallbackHandler]|BaseCallbackManager|None=None",
    "*",
    "tags:list[str]|None=None",
    "metadata:dict[str,Any]|None=None",
    "**kwargs:Any",
    "inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "input:Any",
    "config:RunnableConfig|None=None",
    "*",
    "version:Literal['v1','v2']='v2'",
    "include_names:Sequence[str]|None=None",
    "include_types:Sequence[str]|None=None",
    "include_tags:Sequence[str]|None=None",
    "exclude_names:Sequence[str]|None=None",
    "exclude_types:Sequence[str]|None=None",
    "exclude_tags:Sequence[str]|None=None",
    "**kwargs:Any",
    "inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    "**kwargs:Any",
    "which:ConfigurableField",
    "*",
    "default_key:str='default'",
    "prefix_keys:bool=False",
    "**kwargs:Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]",
    "**kwargs:ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption",
    "choices:Any",
    "prompts:List[str]",
    "params:Dict[str,Any]",
    "token_usage:Dict[str,int]",
    "*",
    "system_fingerprint:str|None=None",
    "text:str",
    "messages:list[BaseMessage]",
    "tools:Sequence|None=None",
    "params:Dict[str,Any]",
    "prompts:List[str]",
    "stop:List[str]|None=None",
    "text:str",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "prompt:str",
    "file_path:Path|str",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "*",
    "on_start:AsyncListener|None=None",
    "on_end:AsyncListener|None=None",
    "on_error:AsyncListener|None=None",
    "config:RunnableConfig|None=None",
    "**kwargs:Any",
    "fallbacks:Sequence[Runnable[Input,Output]],*,exceptions_to_handle:tuple[type[BaseException],...]=(<class'Exception'>,),exception_key:Optional[str]=None",
    "*",
    "on_start:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_end:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_error:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "*,retry_if_exception_type:tuple[type[BaseException],...]=(<class'Exception'>,),wait_exponential_jitter:bool=True,exponential_jitter_params:Optional[ExponentialJitterParams]=None,stop_after_attempt:int=3",
    "schema:dict|type",
    "**kwargs:Any",
    "*",
    "input_type:type[Input]|None=None",
    "output_type:type[Output]|None=None"
  ]
}