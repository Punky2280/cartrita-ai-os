{
  "url": "https://python.langchain.com/api_reference/community/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.max_new_tokens",
  "title": "HuggingFaceTextGenInference#",
  "sections": [
    {
      "type": "li",
      "content": "LangChain Python API Reference"
    },
    {
      "type": "li",
      "content": "langchain-community: 0.3.29"
    },
    {
      "type": "li",
      "content": "HuggingFaceTextGenInference"
    },
    {
      "type": "p",
      "content": "Deprecated since version 0.0.21:Use:class:`~langchain_huggingface.HuggingFaceEndpoint`instead. It will not be removed until langchain-community==1.0."
    },
    {
      "type": "p",
      "content": "HuggingFace text generation API.\n! This class is deprecated, you should use HuggingFaceEndpoint instead !"
    },
    {
      "type": "p",
      "content": "To use, you should have thetext-generationpython package installed and\na text-generation server running."
    },
    {
      "type": "p",
      "content": "HuggingFaceTextGenInference implements the standardRunnableInterface. üèÉ"
    },
    {
      "type": "p",
      "content": "TheRunnableInterfacehas additional methods that are available on runnables, such aswith_config,with_types,with_retry,assign,bind,get_graph, and more."
    },
    {
      "type": "p",
      "content": "Whether to cache the response."
    },
    {
      "type": "li",
      "content": "If true, will use the global cache."
    },
    {
      "type": "p",
      "content": "If true, will use the global cache."
    },
    {
      "type": "li",
      "content": "If false, will not use a cache"
    },
    {
      "type": "p",
      "content": "If false, will not use a cache"
    },
    {
      "type": "li",
      "content": "If None, will use the global cache if it‚Äôs set, otherwise no cache."
    },
    {
      "type": "p",
      "content": "If None, will use the global cache if it‚Äôs set, otherwise no cache."
    },
    {
      "type": "li",
      "content": "If instance ofBaseCache, will use the provided cache."
    },
    {
      "type": "p",
      "content": "If instance ofBaseCache, will use the provided cache."
    },
    {
      "type": "p",
      "content": "Caching is not currently supported for streaming methods of models."
    },
    {
      "type": "p",
      "content": "[DEPRECATED]"
    },
    {
      "type": "p",
      "content": "Callbacks to add to the run trace."
    },
    {
      "type": "p",
      "content": "Optional encoder to use for counting tokens."
    },
    {
      "type": "p",
      "content": "Activate logits sampling"
    },
    {
      "type": "p",
      "content": "text-generation-inference instance base url"
    },
    {
      "type": "p",
      "content": "Maximum number of generated tokens"
    },
    {
      "type": "p",
      "content": "Metadata to add to the run trace."
    },
    {
      "type": "p",
      "content": "Holds any model parameters valid forcallnot explicitly specified"
    },
    {
      "type": "p",
      "content": "The parameter for repetition penalty. 1.0 means no penalty.\nSee [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more details."
    },
    {
      "type": "p",
      "content": "Whether to prepend the prompt to the generated text"
    },
    {
      "type": "p",
      "content": "Random sampling seed"
    },
    {
      "type": "p",
      "content": "Holds any text-generation-inference server parameters not explicitly specified"
    },
    {
      "type": "p",
      "content": "Stop generating tokens if a member ofstop_sequencesis generated"
    },
    {
      "type": "p",
      "content": "Whether to generate a stream of tokens asynchronously"
    },
    {
      "type": "p",
      "content": "Tags to add to the run trace."
    },
    {
      "type": "p",
      "content": "The value used to module the logits distribution."
    },
    {
      "type": "p",
      "content": "Timeout in seconds"
    },
    {
      "type": "p",
      "content": "The number of highest probability vocabulary tokens to keep for\ntop-k-filtering."
    },
    {
      "type": "p",
      "content": "If set to < 1, only the smallest set of most probable tokens with probabilities\nthat add up totop_por higher are kept for generation."
    },
    {
      "type": "p",
      "content": "Truncate inputs tokens to the given size"
    },
    {
      "type": "p",
      "content": "Typical Decoding mass. See [Typical Decoding for Natural Language\nGeneration](https://arxiv.org/abs/2202.00666) for more information."
    },
    {
      "type": "p",
      "content": "Whether to print out response text."
    },
    {
      "type": "p",
      "content": "Watermarking with [A Watermark for Large Language Models]\n(https://arxiv.org/abs/2301.10226)"
    },
    {
      "type": "p",
      "content": "Validate that python package exists in environment."
    },
    {
      "type": "p",
      "content": "values(Dict)"
    },
    {
      "type": "p",
      "content": "Deprecated since version 0.1.7:Useinvoke()instead. It will not be removed until langchain-core==1.0."
    },
    {
      "type": "p",
      "content": "Check Cache and run the LLM on the given prompt and input."
    },
    {
      "type": "li",
      "content": "prompt(str) ‚Äì The prompt to generate from."
    },
    {
      "type": "p",
      "content": "prompt(str) ‚Äì The prompt to generate from."
    },
    {
      "type": "li",
      "content": "stop(list[str]|None) ‚Äì Stop words to use when generating. Model output is cut off at the\nfirst occurrence of any of these substrings."
    },
    {
      "type": "p",
      "content": "stop(list[str]|None) ‚Äì Stop words to use when generating. Model output is cut off at the\nfirst occurrence of any of these substrings."
    },
    {
      "type": "li",
      "content": "callbacks(list[BaseCallbackHandler]|BaseCallbackManager|None) ‚Äì Callbacks to pass through. Used for executing additional\nfunctionality, such as logging or streaming, throughout generation."
    },
    {
      "type": "p",
      "content": "callbacks(list[BaseCallbackHandler]|BaseCallbackManager|None) ‚Äì Callbacks to pass through. Used for executing additional\nfunctionality, such as logging or streaming, throughout generation."
    },
    {
      "type": "li",
      "content": "tags(list[str]|None) ‚Äì List of tags to associate with the prompt."
    },
    {
      "type": "p",
      "content": "tags(list[str]|None) ‚Äì List of tags to associate with the prompt."
    },
    {
      "type": "li",
      "content": "metadata(dict[str,Any]|None) ‚Äì Metadata to associate with the prompt."
    },
    {
      "type": "p",
      "content": "metadata(dict[str,Any]|None) ‚Äì Metadata to associate with the prompt."
    },
    {
      "type": "li",
      "content": "**kwargs(Any) ‚Äì Arbitrary additional keyword arguments. These are usually passed\nto the model provider API call."
    },
    {
      "type": "p",
      "content": "**kwargs(Any) ‚Äì Arbitrary additional keyword arguments. These are usually passed\nto the model provider API call."
    },
    {
      "type": "p",
      "content": "The generated text."
    },
    {
      "type": "p",
      "content": "ValueError‚Äì If the prompt is not a string."
    },
    {
      "type": "p",
      "content": "Default implementation runsainvokein parallel usingasyncio.gather."
    },
    {
      "type": "p",
      "content": "The default implementation ofbatchworks well for IO bound runnables."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they can batch more efficiently;\ne.g., if the underlyingRunnableuses an API which supports a batch mode."
    },
    {
      "type": "li",
      "content": "inputs(list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "**kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A list of outputs from theRunnable."
    },
    {
      "type": "p",
      "content": "Runainvokein parallel on a list of inputs."
    },
    {
      "type": "p",
      "content": "Yields results as they complete."
    },
    {
      "type": "li",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A tuple of the index of the input and the output from theRunnable."
    },
    {
      "type": "p",
      "content": "AsyncIterator[tuple[int,Output| Exception]]"
    },
    {
      "type": "p",
      "content": "Transform a single input into an output."
    },
    {
      "type": "li",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "li",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "Default implementation ofastream, which callsainvoke."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they support streaming output."
    },
    {
      "type": "li",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|None) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|None) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "li",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "AsyncIterator[str]"
    },
    {
      "type": "p",
      "content": "Generate a stream of events."
    },
    {
      "type": "p",
      "content": "Use to create an iterator overStreamEventsthat provide real-time information\nabout the progress of theRunnable, includingStreamEventsfrom intermediate\nresults."
    },
    {
      "type": "p",
      "content": "AStreamEventis a dictionary with the following schema:"
    },
    {
      "type": "li",
      "content": "event:str- Event names are of the format:on_[runnable_type]_(start|stream|end)."
    },
    {
      "type": "p",
      "content": "event:str- Event names are of the format:on_[runnable_type]_(start|stream|end)."
    },
    {
      "type": "li",
      "content": "name:str- The name of theRunnablethat generated the event."
    },
    {
      "type": "p",
      "content": "name:str- The name of theRunnablethat generated the event."
    },
    {
      "type": "li",
      "content": "run_id:str- randomly generated ID associated with the given\nexecution of theRunnablethat emitted the event. A childRunnablethat gets\ninvoked as part of the execution of a parentRunnableis assigned its own\nunique ID."
    },
    {
      "type": "p",
      "content": "run_id:str- randomly generated ID associated with the given\nexecution of theRunnablethat emitted the event. A childRunnablethat gets\ninvoked as part of the execution of a parentRunnableis assigned its own\nunique ID."
    },
    {
      "type": "li",
      "content": "parent_ids:list[str]- The IDs of the parent runnables that generated\nthe event. The rootRunnablewill have an empty list. The order of the parent\nIDs is from the root to the immediate parent. Only available for v2 version of\nthe API. The v1 version of the API will return an empty list."
    },
    {
      "type": "p",
      "content": "parent_ids:list[str]- The IDs of the parent runnables that generated\nthe event. The rootRunnablewill have an empty list. The order of the parent\nIDs is from the root to the immediate parent. Only available for v2 version of\nthe API. The v1 version of the API will return an empty list."
    },
    {
      "type": "li",
      "content": "tags:Optional[list[str]]- The tags of theRunnablethat generated\nthe event."
    },
    {
      "type": "p",
      "content": "tags:Optional[list[str]]- The tags of theRunnablethat generated\nthe event."
    },
    {
      "type": "li",
      "content": "metadata:Optional[dict[str, Any]]- The metadata of theRunnablethat\ngenerated the event."
    },
    {
      "type": "p",
      "content": "metadata:Optional[dict[str, Any]]- The metadata of theRunnablethat\ngenerated the event."
    },
    {
      "type": "li",
      "content": "data:dict[str, Any]"
    },
    {
      "type": "p",
      "content": "data:dict[str, Any]"
    },
    {
      "type": "p",
      "content": "Below is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table."
    },
    {
      "type": "p",
      "content": "This reference table is for the v2 version of the schema."
    },
    {
      "type": "p",
      "content": "on_chat_model_start"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "{\"messages\":[[SystemMessage,HumanMessage]]}"
    },
    {
      "type": "p",
      "content": "on_chat_model_stream"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "AIMessageChunk(content=\"hello\")"
    },
    {
      "type": "p",
      "content": "on_chat_model_end"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "{\"messages\":[[SystemMessage,HumanMessage]]}"
    },
    {
      "type": "p",
      "content": "AIMessageChunk(content=\"helloworld\")"
    },
    {
      "type": "p",
      "content": "on_llm_start"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "{'input':'hello'}"
    },
    {
      "type": "p",
      "content": "on_llm_stream"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "``‚ÄôHello‚Äô ``"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "'Hellohuman!'"
    },
    {
      "type": "p",
      "content": "on_chain_start"
    },
    {
      "type": "p",
      "content": "format_docs"
    },
    {
      "type": "p",
      "content": "on_chain_stream"
    },
    {
      "type": "p",
      "content": "format_docs"
    },
    {
      "type": "p",
      "content": "'helloworld!,goodbyeworld!'"
    },
    {
      "type": "p",
      "content": "on_chain_end"
    },
    {
      "type": "p",
      "content": "format_docs"
    },
    {
      "type": "p",
      "content": "[Document(...)]"
    },
    {
      "type": "p",
      "content": "'helloworld!,goodbyeworld!'"
    },
    {
      "type": "p",
      "content": "on_tool_start"
    },
    {
      "type": "p",
      "content": "{\"x\":1,\"y\":\"2\"}"
    },
    {
      "type": "p",
      "content": "on_tool_end"
    },
    {
      "type": "p",
      "content": "{\"x\":1,\"y\":\"2\"}"
    },
    {
      "type": "p",
      "content": "on_retriever_start"
    },
    {
      "type": "p",
      "content": "[retriever name]"
    },
    {
      "type": "p",
      "content": "{\"query\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "on_retriever_end"
    },
    {
      "type": "p",
      "content": "[retriever name]"
    },
    {
      "type": "p",
      "content": "{\"query\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "[Document(...),..]"
    },
    {
      "type": "p",
      "content": "on_prompt_start"
    },
    {
      "type": "p",
      "content": "[template_name]"
    },
    {
      "type": "p",
      "content": "{\"question\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "on_prompt_end"
    },
    {
      "type": "p",
      "content": "[template_name]"
    },
    {
      "type": "p",
      "content": "{\"question\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "ChatPromptValue(messages:[SystemMessage,...])"
    },
    {
      "type": "p",
      "content": "In addition to the standard events, users can also dispatch custom events (see example below)."
    },
    {
      "type": "p",
      "content": "Custom events will be only be surfaced with in the v2 version of the API!"
    },
    {
      "type": "p",
      "content": "A custom event has following format:"
    },
    {
      "type": "p",
      "content": "Description"
    },
    {
      "type": "p",
      "content": "A user defined name for the event."
    },
    {
      "type": "p",
      "content": "The data associated with the event. This can be anything, though we suggest making it JSON serializable."
    },
    {
      "type": "p",
      "content": "Here are declarations associated with the standard events shown above:"
    },
    {
      "type": "p",
      "content": "format_docs:"
    },
    {
      "type": "p",
      "content": "Example: Dispatch Custom Event"
    },
    {
      "type": "li",
      "content": "input(Any) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(Any) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(Optional[RunnableConfig]) ‚Äì The config to use for theRunnable."
    },
    {
      "type": "p",
      "content": "config(Optional[RunnableConfig]) ‚Äì The config to use for theRunnable."
    },
    {
      "type": "li",
      "content": "version(Literal['v1','v2']) ‚Äì The version of the schema to use either'v2'or'v1'.\nUsers should use'v2'.'v1'is for backwards compatibility and will be deprecated\nin 0.4.0.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in'v2'."
    },
    {
      "type": "p",
      "content": "version(Literal['v1','v2']) ‚Äì The version of the schema to use either'v2'or'v1'.\nUsers should use'v2'.'v1'is for backwards compatibility and will be deprecated\nin 0.4.0.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in'v2'."
    },
    {
      "type": "li",
      "content": "include_names(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching names."
    },
    {
      "type": "p",
      "content": "include_names(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching names."
    },
    {
      "type": "li",
      "content": "include_types(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching types."
    },
    {
      "type": "p",
      "content": "include_types(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching types."
    },
    {
      "type": "li",
      "content": "include_tags(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching tags."
    },
    {
      "type": "p",
      "content": "include_tags(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching tags."
    },
    {
      "type": "li",
      "content": "exclude_names(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching names."
    },
    {
      "type": "p",
      "content": "exclude_names(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching names."
    },
    {
      "type": "li",
      "content": "exclude_types(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching types."
    },
    {
      "type": "p",
      "content": "exclude_types(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching types."
    },
    {
      "type": "li",
      "content": "exclude_tags(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching tags."
    },
    {
      "type": "p",
      "content": "exclude_tags(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching tags."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable.\nThese will be passed toastream_logas this implementation\nofastream_eventsis built on top ofastream_log."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable.\nThese will be passed toastream_logas this implementation\nofastream_eventsis built on top ofastream_log."
    },
    {
      "type": "p",
      "content": "An async stream ofStreamEvents."
    },
    {
      "type": "p",
      "content": "NotImplementedError‚Äì If the version is not'v1'or'v2'."
    },
    {
      "type": "p",
      "content": "AsyncIterator[StreamEvent]"
    },
    {
      "type": "p",
      "content": "Default implementation runs invoke in parallel using a thread pool executor."
    },
    {
      "type": "p",
      "content": "The default implementation of batch works well for IO bound runnables."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they can batch more efficiently;\ne.g., if the underlyingRunnableuses an API which supports a batch mode."
    },
    {
      "type": "li",
      "content": "inputs(list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work\nto do in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work\nto do in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "**kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A list of outputs from theRunnable."
    },
    {
      "type": "p",
      "content": "Runinvokein parallel on a list of inputs."
    },
    {
      "type": "p",
      "content": "Yields results as they complete."
    },
    {
      "type": "li",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "**kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "Tuples of the index of the input and the output from theRunnable."
    },
    {
      "type": "p",
      "content": "Iterator[tuple[int,Output| Exception]]"
    },
    {
      "type": "p",
      "content": "Bind arguments to aRunnable, returning a newRunnable."
    },
    {
      "type": "p",
      "content": "Useful when aRunnablein a chain requires an argument that is not\nin the output of the previousRunnableor included in the user input."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì The arguments to bind to theRunnable."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the arguments bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input,Output]"
    },
    {
      "type": "p",
      "content": "Configure alternatives forRunnablesthat can be set at runtime."
    },
    {
      "type": "li",
      "content": "which(ConfigurableField) ‚Äì TheConfigurableFieldinstance that will be used to select the\nalternative."
    },
    {
      "type": "p",
      "content": "which(ConfigurableField) ‚Äì TheConfigurableFieldinstance that will be used to select the\nalternative."
    },
    {
      "type": "li",
      "content": "default_key(str) ‚Äì The default key to use if no alternative is selected.\nDefaults to'default'."
    },
    {
      "type": "p",
      "content": "default_key(str) ‚Äì The default key to use if no alternative is selected.\nDefaults to'default'."
    },
    {
      "type": "li",
      "content": "prefix_keys(bool) ‚Äì Whether to prefix the keys with theConfigurableFieldid.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "prefix_keys(bool) ‚Äì Whether to prefix the keys with theConfigurableFieldid.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]) ‚Äì A dictionary of keys toRunnableinstances or callables that\nreturnRunnableinstances."
    },
    {
      "type": "p",
      "content": "**kwargs(Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]) ‚Äì A dictionary of keys toRunnableinstances or callables that\nreturnRunnableinstances."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the alternatives configured."
    },
    {
      "type": "p",
      "content": "RunnableSerializable"
    },
    {
      "type": "p",
      "content": "Configure particularRunnablefields at runtime."
    },
    {
      "type": "p",
      "content": "**kwargs(ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption) ‚Äì A dictionary ofConfigurableFieldinstances to configure."
    },
    {
      "type": "p",
      "content": "ValueError‚Äì If a configuration key is not found in theRunnable."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the fields configured."
    },
    {
      "type": "p",
      "content": "RunnableSerializable"
    },
    {
      "type": "p",
      "content": "Get the number of tokens present in the text."
    },
    {
      "type": "p",
      "content": "Useful for checking if an input fits in a model‚Äôs context window."
    },
    {
      "type": "p",
      "content": "text(str) ‚Äì The string input to tokenize."
    },
    {
      "type": "p",
      "content": "The integer number of tokens in the text."
    },
    {
      "type": "p",
      "content": "Get the number of tokens in the messages."
    },
    {
      "type": "p",
      "content": "Useful for checking if an input fits in a model‚Äôs context window."
    },
    {
      "type": "p",
      "content": "The base implementation ofget_num_tokens_from_messagesignores tool\nschemas."
    },
    {
      "type": "li",
      "content": "messages(list[BaseMessage]) ‚Äì The message inputs to tokenize."
    },
    {
      "type": "p",
      "content": "messages(list[BaseMessage]) ‚Äì The message inputs to tokenize."
    },
    {
      "type": "li",
      "content": "tools(Sequence|None) ‚Äì If provided, sequence of dict,BaseModel, function, orBaseToolsto be converted to tool schemas."
    },
    {
      "type": "p",
      "content": "tools(Sequence|None) ‚Äì If provided, sequence of dict,BaseModel, function, orBaseToolsto be converted to tool schemas."
    },
    {
      "type": "p",
      "content": "The sum of the number of tokens across the messages."
    },
    {
      "type": "p",
      "content": "Return the ordered ids of the tokens in a text."
    },
    {
      "type": "p",
      "content": "text(str) ‚Äì The string input to tokenize."
    },
    {
      "type": "p",
      "content": "A list of ids corresponding to the tokens in the text, in order they occur\nin the text."
    },
    {
      "type": "p",
      "content": "Transform a single input into an output."
    },
    {
      "type": "li",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "li",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "Save the LLM."
    },
    {
      "type": "p",
      "content": "file_path(Path|str) ‚Äì Path to file to save the LLM to."
    },
    {
      "type": "p",
      "content": "ValueError‚Äì If the file path is not a string or Path object."
    },
    {
      "type": "p",
      "content": "Default implementation ofstream, which callsinvoke."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they support streaming output."
    },
    {
      "type": "li",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|None) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|None) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "li",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "stop(list[str]|None)"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "Iterator[str]"
    },
    {
      "type": "p",
      "content": "Bind async lifecycle listeners to aRunnable."
    },
    {
      "type": "p",
      "content": "Returns a newRunnable."
    },
    {
      "type": "p",
      "content": "The Run object contains information about the run, including itsid,type,input,output,error,start_time,end_time, and\nany tags or metadata added to the run."
    },
    {
      "type": "li",
      "content": "on_start(Optional[AsyncListener]) ‚Äì Called asynchronously before theRunnablestarts running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_start(Optional[AsyncListener]) ‚Äì Called asynchronously before theRunnablestarts running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_end(Optional[AsyncListener]) ‚Äì Called asynchronously after theRunnablefinishes running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_end(Optional[AsyncListener]) ‚Äì Called asynchronously after theRunnablefinishes running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_error(Optional[AsyncListener]) ‚Äì Called asynchronously if theRunnablethrows an error,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_error(Optional[AsyncListener]) ‚Äì Called asynchronously if theRunnablethrows an error,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the listeners bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input, Output]"
    },
    {
      "type": "p",
      "content": "Bind config to aRunnable, returning a newRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|None) ‚Äì The config to bind to theRunnable."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|None) ‚Äì The config to bind to theRunnable."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the config bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input,Output]"
    },
    {
      "type": "p",
      "content": "Add fallbacks to aRunnable, returning a newRunnable."
    },
    {
      "type": "p",
      "content": "The newRunnablewill try the originalRunnable, and then each fallback\nin order, upon failures."
    },
    {
      "type": "li",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "p",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "li",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle.\nDefaults to(Exception,)."
    },
    {
      "type": "p",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle.\nDefaults to(Exception,)."
    },
    {
      "type": "li",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input. Defaults to None."
    },
    {
      "type": "p",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input. Defaults to None."
    },
    {
      "type": "p",
      "content": "A newRunnablethat will try the originalRunnable, and then each\nfallback in order, upon failures."
    },
    {
      "type": "p",
      "content": "RunnableWithFallbacksT[Input, Output]"
    },
    {
      "type": "li",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "p",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "li",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle."
    },
    {
      "type": "p",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle."
    },
    {
      "type": "li",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input."
    },
    {
      "type": "p",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input."
    },
    {
      "type": "p",
      "content": "A newRunnablethat will try the originalRunnable, and then each\nfallback in order, upon failures."
    },
    {
      "type": "p",
      "content": "RunnableWithFallbacksT[Input, Output]"
    },
    {
      "type": "p",
      "content": "Bind lifecycle listeners to aRunnable, returning a newRunnable."
    },
    {
      "type": "p",
      "content": "The Run object contains information about the run, including itsid,type,input,output,error,start_time,end_time, and\nany tags or metadata added to the run."
    },
    {
      "type": "li",
      "content": "on_start(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called before theRunnablestarts running, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_start(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called before theRunnablestarts running, with theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_end(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called after theRunnablefinishes running, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_end(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called after theRunnablefinishes running, with theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_error(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called if theRunnablethrows an error, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_error(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called if theRunnablethrows an error, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the listeners bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input, Output]"
    },
    {
      "type": "p",
      "content": "Create a new Runnable that retries the original Runnable on exceptions."
    },
    {
      "type": "li",
      "content": "retry_if_exception_type(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to retry on.\nDefaults to (Exception,)."
    },
    {
      "type": "p",
      "content": "retry_if_exception_type(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to retry on.\nDefaults to (Exception,)."
    },
    {
      "type": "li",
      "content": "wait_exponential_jitter(bool) ‚Äì Whether to add jitter to the wait\ntime between retries. Defaults to True."
    },
    {
      "type": "p",
      "content": "wait_exponential_jitter(bool) ‚Äì Whether to add jitter to the wait\ntime between retries. Defaults to True."
    },
    {
      "type": "li",
      "content": "stop_after_attempt(int) ‚Äì The maximum number of attempts to make before\ngiving up. Defaults to 3."
    },
    {
      "type": "p",
      "content": "stop_after_attempt(int) ‚Äì The maximum number of attempts to make before\ngiving up. Defaults to 3."
    },
    {
      "type": "li",
      "content": "exponential_jitter_params(Optional[ExponentialJitterParams]) ‚Äì Parameters fortenacity.wait_exponential_jitter. Namely:initial,max,exp_base, andjitter(all float values)."
    },
    {
      "type": "p",
      "content": "exponential_jitter_params(Optional[ExponentialJitterParams]) ‚Äì Parameters fortenacity.wait_exponential_jitter. Namely:initial,max,exp_base, andjitter(all float values)."
    },
    {
      "type": "p",
      "content": "A new Runnable that retries the original Runnable on exceptions."
    },
    {
      "type": "p",
      "content": "Runnable[Input, Output]"
    },
    {
      "type": "p",
      "content": "Not implemented on this class."
    },
    {
      "type": "li",
      "content": "schema(dict|type)"
    },
    {
      "type": "p",
      "content": "schema(dict|type)"
    },
    {
      "type": "li",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "Runnable[PromptValue| str |Sequence[BaseMessage| list[str] | tuple[str, str] | str | dict[str,Any]], dict |BaseModel]"
    },
    {
      "type": "p",
      "content": "Bind input and output types to aRunnable, returning a newRunnable."
    },
    {
      "type": "li",
      "content": "input_type(type[Input]|None) ‚Äì The input type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "input_type(type[Input]|None) ‚Äì The input type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "li",
      "content": "output_type(type[Output]|None) ‚Äì The output type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "output_type(type[Output]|None) ‚Äì The output type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "A new Runnable with the types bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input,Output]"
    },
    {
      "type": "p",
      "content": "Examples using HuggingFaceTextGenInference"
    },
    {
      "type": "li",
      "content": "HuggingFaceTextGenInferenceasync_clientcachecallback_managercallbacksclientcustom_get_token_idsdo_sampleinference_server_urlmax_new_tokensmetadatamodel_kwargsrepetition_penaltyreturn_full_textseedserver_kwargsstop_sequencesstreamingtagstemperaturetimeouttop_ktop_ptruncatetypical_pverbosewatermarkvalidate_environment()__call__()abatch()abatch_as_completed()ainvoke()astream()astream_events()batch()batch_as_completed()bind()configurable_alternatives()configurable_fields()get_num_tokens()get_num_tokens_from_messages()get_token_ids()invoke()save()stream()with_alisteners()with_config()with_fallbacks()with_listeners()with_retry()with_structured_output()with_types()"
    },
    {
      "type": "li",
      "content": "async_client"
    },
    {
      "type": "li",
      "content": "callback_manager"
    },
    {
      "type": "li",
      "content": "custom_get_token_ids"
    },
    {
      "type": "li",
      "content": "inference_server_url"
    },
    {
      "type": "li",
      "content": "max_new_tokens"
    },
    {
      "type": "li",
      "content": "model_kwargs"
    },
    {
      "type": "li",
      "content": "repetition_penalty"
    },
    {
      "type": "li",
      "content": "return_full_text"
    },
    {
      "type": "li",
      "content": "server_kwargs"
    },
    {
      "type": "li",
      "content": "stop_sequences"
    },
    {
      "type": "li",
      "content": "temperature"
    },
    {
      "type": "li",
      "content": "validate_environment()"
    },
    {
      "type": "li",
      "content": "abatch_as_completed()"
    },
    {
      "type": "li",
      "content": "astream_events()"
    },
    {
      "type": "li",
      "content": "batch_as_completed()"
    },
    {
      "type": "li",
      "content": "configurable_alternatives()"
    },
    {
      "type": "li",
      "content": "configurable_fields()"
    },
    {
      "type": "li",
      "content": "get_num_tokens()"
    },
    {
      "type": "li",
      "content": "get_num_tokens_from_messages()"
    },
    {
      "type": "li",
      "content": "get_token_ids()"
    },
    {
      "type": "li",
      "content": "with_alisteners()"
    },
    {
      "type": "li",
      "content": "with_config()"
    },
    {
      "type": "li",
      "content": "with_fallbacks()"
    },
    {
      "type": "li",
      "content": "with_listeners()"
    },
    {
      "type": "li",
      "content": "with_retry()"
    },
    {
      "type": "li",
      "content": "with_structured_output()"
    },
    {
      "type": "li",
      "content": "with_types()"
    }
  ],
  "code_examples": [
    "llms",
    "LLM",
    ":class:`~langchain_huggingface.HuggingFaceEndpoint`",
    "# Basic Example (no streaming)llm=HuggingFaceTextGenInference(inference_server_url=\"http://localhost:8010/\",max_new_tokens=512,top_k=10,top_p=0.95,typical_p=0.95,temperature=0.01,repetition_penalty=1.03,)print(llm.invoke(\"What is Deep Learning?\"))# noqa: T201# Streaming response examplefromlangchain_community.callbacksimportstreaming_stdoutcallbacks=[streaming_stdout.StreamingStdOutCallbackHandler()]llm=HuggingFaceTextGenInference(inference_server_url=\"http://localhost:8010/\",max_new_tokens=512,top_k=10,top_p=0.95,typical_p=0.95,temperature=0.01,repetition_penalty=1.03,callbacks=callbacks,streaming=True)print(llm.invoke(\"What is Deep Learning?\"))# noqa: T201",
    "RunnableInterface",
    "RunnableInterface",
    "with_config",
    "with_types",
    "with_retry",
    "assign",
    "bind",
    "get_graph",
    "BaseCache",
    "invoke()",
    "ainvoke",
    "asyncio.gather",
    "batch",
    "Runnable",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "ainvoke",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "astream",
    "ainvoke",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "StreamEvents",
    "Runnable",
    "StreamEvents",
    "StreamEvent",
    "event",
    "on_[runnable_type]_(start|stream|end)",
    "name",
    "Runnable",
    "run_id",
    "Runnable",
    "Runnable",
    "Runnable",
    "parent_ids",
    "Runnable",
    "tags",
    "Runnable",
    "metadata",
    "Runnable",
    "data",
    "on_chat_model_start",
    "{\"messages\":[[SystemMessage,HumanMessage]]}",
    "on_chat_model_stream",
    "AIMessageChunk(content=\"hello\")",
    "on_chat_model_end",
    "{\"messages\":[[SystemMessage,HumanMessage]]}",
    "AIMessageChunk(content=\"helloworld\")",
    "on_llm_start",
    "{'input':'hello'}",
    "on_llm_stream",
    "on_llm_end",
    "'Hellohuman!'",
    "on_chain_start",
    "on_chain_stream",
    "'helloworld!,goodbyeworld!'",
    "on_chain_end",
    "[Document(...)]",
    "'helloworld!,goodbyeworld!'",
    "on_tool_start",
    "{\"x\":1,\"y\":\"2\"}",
    "on_tool_end",
    "{\"x\":1,\"y\":\"2\"}",
    "on_retriever_start",
    "{\"query\":\"hello\"}",
    "on_retriever_end",
    "{\"query\":\"hello\"}",
    "[Document(...),..]",
    "on_prompt_start",
    "{\"question\":\"hello\"}",
    "on_prompt_end",
    "{\"question\":\"hello\"}",
    "ChatPromptValue(messages:[SystemMessage,...])",
    "format_docs",
    "defformat_docs(docs:list[Document])->str:'''Format the docs.'''return\", \".join([doc.page_contentfordocindocs])format_docs=RunnableLambda(format_docs)",
    "some_tool",
    "@tooldefsome_tool(x:int,y:str)->dict:'''Some_tool.'''return{\"x\":x,\"y\":y}",
    "prompt",
    "template=ChatPromptTemplate.from_messages([(\"system\",\"You are Cat Agent 007\"),(\"human\",\"{question}\")]).with_config({\"run_name\":\"my_template\",\"tags\":[\"my_template\"]})",
    "fromlangchain_core.runnablesimportRunnableLambdaasyncdefreverse(s:str)->str:returns[::-1]chain=RunnableLambda(func=reverse)events=[eventasyncforeventinchain.astream_events(\"hello\",version=\"v2\")]# will produce the following events (run_id, and parent_ids# has been omitted for brevity):[{\"data\":{\"input\":\"hello\"},\"event\":\"on_chain_start\",\"metadata\":{},\"name\":\"reverse\",\"tags\":[],},{\"data\":{\"chunk\":\"olleh\"},\"event\":\"on_chain_stream\",\"metadata\":{},\"name\":\"reverse\",\"tags\":[],},{\"data\":{\"output\":\"olleh\"},\"event\":\"on_chain_end\",\"metadata\":{},\"name\":\"reverse\",\"tags\":[],},]",
    "fromlangchain_core.callbacks.managerimport(adispatch_custom_event,)fromlangchain_core.runnablesimportRunnableLambda,RunnableConfigimportasyncioasyncdefslow_thing(some_input:str,config:RunnableConfig)->str:\"\"\"Do something that takes a long time.\"\"\"awaitasyncio.sleep(1)# Placeholder for some slow operationawaitadispatch_custom_event(\"progress_event\",{\"message\":\"Finished step 1 of 3\"},config=config# Must be included for python < 3.10)awaitasyncio.sleep(1)# Placeholder for some slow operationawaitadispatch_custom_event(\"progress_event\",{\"message\":\"Finished step 2 of 3\"},config=config# Must be included for python < 3.10)awaitasyncio.sleep(1)# Placeholder for some slow operationreturn\"Done\"slow_thing=RunnableLambda(slow_thing)asyncforeventinslow_thing.astream_events(\"some_input\",version=\"v2\"):print(event)",
    "Runnable",
    "Runnable",
    "'v2'",
    "'v1'",
    "'v2'",
    "'v1'",
    "'v2'",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnable",
    "astream_log",
    "astream_events",
    "astream_log",
    "StreamEvents",
    "'v1'",
    "'v2'",
    "Runnable",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "invoke",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "fromlangchain_ollamaimportChatOllamafromlangchain_core.output_parsersimportStrOutputParserllm=ChatOllama(model=\"llama2\")# Without bind.chain=llm|StrOutputParser()chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")# Output is 'One two three four five.'# With bind.chain=llm.bind(stop=[\"three\"])|StrOutputParser()chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")# Output is 'One two'",
    "Runnables",
    "ConfigurableField",
    "'default'",
    "ConfigurableField",
    "Runnable",
    "Runnable",
    "Runnable",
    "fromlangchain_anthropicimportChatAnthropicfromlangchain_core.runnables.utilsimportConfigurableFieldfromlangchain_openaiimportChatOpenAImodel=ChatAnthropic(model_name=\"claude-3-7-sonnet-20250219\").configurable_alternatives(ConfigurableField(id=\"llm\"),default_key=\"anthropic\",openai=ChatOpenAI(),)# uses the default model ChatAnthropicprint(model.invoke(\"which organization created you?\").content)# uses ChatOpenAIprint(model.with_config(configurable={\"llm\":\"openai\"}).invoke(\"which organization created you?\").content)",
    "Runnable",
    "ConfigurableField",
    "Runnable",
    "Runnable",
    "fromlangchain_core.runnablesimportConfigurableFieldfromlangchain_openaiimportChatOpenAImodel=ChatOpenAI(max_tokens=20).configurable_fields(max_tokens=ConfigurableField(id=\"output_token_number\",name=\"Max tokens in the output\",description=\"The maximum number of tokens in the output\",))# max_tokens = 20print(\"max_tokens_20: \",model.invoke(\"tell me something about chess\").content)# max_tokens = 200print(\"max_tokens_200: \",model.with_config(configurable={\"output_token_number\":200}).invoke(\"tell me something about chess\").content,)",
    "get_num_tokens_from_messages",
    "BaseModel",
    "BaseTools",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "llm.save(file_path=\"path/llm.yaml\")",
    "stream",
    "invoke",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "id",
    "type",
    "input",
    "output",
    "error",
    "start_time",
    "end_time",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "fromlangchain_core.runnablesimportRunnableLambda,Runnablefromdatetimeimportdatetime,timezoneimporttimeimportasynciodefformat_t(timestamp:float)->str:returndatetime.fromtimestamp(timestamp,tz=timezone.utc).isoformat()asyncdeftest_runnable(time_to_sleep:int):print(f\"Runnable[{time_to_sleep}s]: starts at{format_t(time.time())}\")awaitasyncio.sleep(time_to_sleep)print(f\"Runnable[{time_to_sleep}s]: ends at{format_t(time.time())}\")asyncdeffn_start(run_obj:Runnable):print(f\"on start callback starts at{format_t(time.time())}\")awaitasyncio.sleep(3)print(f\"on start callback ends at{format_t(time.time())}\")asyncdeffn_end(run_obj:Runnable):print(f\"on end callback starts at{format_t(time.time())}\")awaitasyncio.sleep(2)print(f\"on end callback ends at{format_t(time.time())}\")runnable=RunnableLambda(test_runnable).with_alisteners(on_start=fn_start,on_end=fn_end)asyncdefconcurrent_runs():awaitasyncio.gather(runnable.ainvoke(2),runnable.ainvoke(3))asyncio.run(concurrent_runs())Result:onstartcallbackstartsat2025-03-01T07:05:22.875378+00:00onstartcallbackstartsat2025-03-01T07:05:22.875495+00:00onstartcallbackendsat2025-03-01T07:05:25.878862+00:00onstartcallbackendsat2025-03-01T07:05:25.878947+00:00Runnable[2s]:startsat2025-03-01T07:05:25.879392+00:00Runnable[3s]:startsat2025-03-01T07:05:25.879804+00:00Runnable[2s]:endsat2025-03-01T07:05:27.881998+00:00onendcallbackstartsat2025-03-01T07:05:27.882360+00:00Runnable[3s]:endsat2025-03-01T07:05:28.881737+00:00onendcallbackstartsat2025-03-01T07:05:28.882428+00:00onendcallbackendsat2025-03-01T07:05:29.883893+00:00onendcallbackendsat2025-03-01T07:05:30.884831+00:00",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "(Exception,)",
    "Runnable",
    "Runnable",
    "Runnable",
    "fromtypingimportIteratorfromlangchain_core.runnablesimportRunnableGeneratordef_generate_immediate_error(input:Iterator)->Iterator[str]:raiseValueError()yield\"\"def_generate(input:Iterator)->Iterator[str]:yield from\"foo bar\"runnable=RunnableGenerator(_generate_immediate_error).with_fallbacks([RunnableGenerator(_generate)])print(\"\".join(runnable.stream({})))# foo bar",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "id",
    "type",
    "input",
    "output",
    "error",
    "start_time",
    "end_time",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "fromlangchain_core.runnablesimportRunnableLambdafromlangchain_core.tracers.schemasimportRunimporttimedeftest_runnable(time_to_sleep:int):time.sleep(time_to_sleep)deffn_start(run_obj:Run):print(\"start_time:\",run_obj.start_time)deffn_end(run_obj:Run):print(\"end_time:\",run_obj.end_time)chain=RunnableLambda(test_runnable).with_listeners(on_start=fn_start,on_end=fn_end)chain.invoke(2)",
    "tenacity.wait_exponential_jitter",
    "initial",
    "max",
    "exp_base",
    "jitter",
    "fromlangchain_core.runnablesimportRunnableLambdacount=0def_lambda(x:int)->None:globalcountcount=count+1ifx==1:raiseValueError(\"x is 1\")else:passrunnable=RunnableLambda(_lambda)try:runnable.with_retry(stop_after_attempt=2,retry_if_exception_type=(ValueError,),).invoke(1)exceptValueError:passassertcount==2",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "HuggingFaceTextGenInference",
    "async_client",
    "cache",
    "callback_manager",
    "callbacks",
    "client",
    "custom_get_token_ids",
    "do_sample",
    "inference_server_url",
    "max_new_tokens",
    "metadata",
    "model_kwargs",
    "repetition_penalty",
    "return_full_text",
    "seed",
    "server_kwargs",
    "stop_sequences",
    "streaming",
    "tags",
    "temperature",
    "timeout",
    "top_k",
    "top_p",
    "truncate",
    "typical_p",
    "verbose",
    "watermark",
    "validate_environment()",
    "__call__()",
    "abatch()",
    "abatch_as_completed()",
    "ainvoke()",
    "astream()",
    "astream_events()",
    "batch()",
    "batch_as_completed()",
    "bind()",
    "configurable_alternatives()",
    "configurable_fields()",
    "get_num_tokens()",
    "get_num_tokens_from_messages()",
    "get_token_ids()",
    "invoke()",
    "save()",
    "stream()",
    "with_alisteners()",
    "with_config()",
    "with_fallbacks()",
    "with_listeners()",
    "with_retry()",
    "with_structured_output()",
    "with_types()"
  ],
  "api_signatures": [
    "classlangchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference[source]#",
    "langchain_community.llms.huggingface_text_gen_inference.",
    "HuggingFaceTextGenInference",
    "paramasync_client:Any=None#",
    "async_client",
    "paramcache:BaseCache|bool|None=None#",
    "cache",
    "paramcallback_manager:BaseCallbackManager|None=None#",
    "callback_manager",
    "paramcallbacks:Callbacks=None#",
    "callbacks",
    "paramclient:Any=None#",
    "client",
    "paramcustom_get_token_ids:Callable[[str],list[int]]|None=None#",
    "custom_get_token_ids",
    "paramdo_sample:bool=False#",
    "do_sample",
    "paraminference_server_url:str=''#",
    "inference_server_url",
    "parammax_new_tokens:int=512#",
    "max_new_tokens",
    "parammetadata:dict[str,Any]|None=None#",
    "metadata",
    "parammodel_kwargs:Dict[str,Any][Optional]#",
    "model_kwargs",
    "paramrepetition_penalty:float|None=None#",
    "repetition_penalty",
    "paramreturn_full_text:bool=False#",
    "return_full_text",
    "paramseed:int|None=None#",
    "seed",
    "paramserver_kwargs:Dict[str,Any][Optional]#",
    "server_kwargs",
    "paramstop_sequences:List[str][Optional]#",
    "stop_sequences",
    "paramstreaming:bool=False#",
    "streaming",
    "paramtags:list[str]|None=None#",
    "tags",
    "paramtemperature:float|None=0.8#",
    "temperature",
    "paramtimeout:int=120#",
    "timeout",
    "paramtop_k:int|None=None#",
    "top_k",
    "paramtop_p:float|None=0.95#",
    "top_p",
    "paramtruncate:int|None=None#",
    "truncate",
    "paramtypical_p:float|None=0.95#",
    "typical_p",
    "paramverbose:bool[Optional]#",
    "verbose",
    "paramwatermark:bool=False#",
    "watermark",
    "classmethodvalidate_environment(values:Dict,)‚ÜíDict[source]#",
    "validate_environment",
    "(",
    "values:Dict",
    ")",
    "‚ÜíDict",
    "‚Üí",
    "Dict",
    "__call__(prompt:str,stop:list[str]|None=None,callbacks:list[BaseCallbackHandler]|BaseCallbackManager|None=None,*,tags:list[str]|None=None,metadata:dict[str,Any]|None=None,**kwargs:Any,)‚Üístr#",
    "__call__",
    "(",
    "prompt:str",
    "stop:list[str]|None=None",
    "callbacks:list[BaseCallbackHandler]|BaseCallbackManager|None=None",
    "*",
    "tags:list[str]|None=None",
    "metadata:dict[str,Any]|None=None",
    "**kwargs:Any",
    ")",
    "‚Üístr",
    "‚Üí",
    "str",
    "asyncabatch(inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]],config:RunnableConfig|list[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any,)‚Üílist[str]#",
    "abatch",
    "(",
    "inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any",
    ")",
    "‚Üílist[str]",
    "‚Üí",
    "list[str]",
    "asyncabatch_as_completed(inputs:Sequence[Input],config:RunnableConfig|Sequence[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any|None,)‚ÜíAsyncIterator[tuple[int,Output|Exception]]#",
    "abatch_as_completed",
    "(",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    ")",
    "‚ÜíAsyncIterator[tuple[int,Output|Exception]]",
    "‚Üí",
    "AsyncIterator[tuple[int,Output|Exception]]",
    "asyncainvoke(input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚Üístr#",
    "ainvoke",
    "(",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚Üístr",
    "‚Üí",
    "str",
    "asyncastream(input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚ÜíAsyncIterator[str]#",
    "astream",
    "(",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíAsyncIterator[str]",
    "‚Üí",
    "AsyncIterator[str]",
    "asyncastream_events(input:Any,config:RunnableConfig|None=None,*,version:Literal['v1','v2']='v2',include_names:Sequence[str]|None=None,include_types:Sequence[str]|None=None,include_tags:Sequence[str]|None=None,exclude_names:Sequence[str]|None=None,exclude_types:Sequence[str]|None=None,exclude_tags:Sequence[str]|None=None,**kwargs:Any,)‚ÜíAsyncIterator[StreamEvent]#",
    "astream_events",
    "(",
    "input:Any",
    "config:RunnableConfig|None=None",
    "*",
    "version:Literal['v1','v2']='v2'",
    "include_names:Sequence[str]|None=None",
    "include_types:Sequence[str]|None=None",
    "include_tags:Sequence[str]|None=None",
    "exclude_names:Sequence[str]|None=None",
    "exclude_types:Sequence[str]|None=None",
    "exclude_tags:Sequence[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíAsyncIterator[StreamEvent]",
    "‚Üí",
    "AsyncIterator[StreamEvent]",
    "batch(inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]],config:RunnableConfig|list[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any,)‚Üílist[str]#",
    "batch",
    "(",
    "inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any",
    ")",
    "‚Üílist[str]",
    "‚Üí",
    "list[str]",
    "batch_as_completed(inputs:Sequence[Input],config:RunnableConfig|Sequence[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any|None,)‚ÜíIterator[tuple[int,Output|Exception]]#",
    "batch_as_completed",
    "(",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    ")",
    "‚ÜíIterator[tuple[int,Output|Exception]]",
    "‚Üí",
    "Iterator[tuple[int,Output|Exception]]",
    "bind(**kwargs:Any,)‚ÜíRunnable[Input,Output]#",
    "bind",
    "(",
    "**kwargs:Any",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "configurable_alternatives(which:ConfigurableField,*,default_key:str='default',prefix_keys:bool=False,**kwargs:Runnable[Input,Output]|Callable[[],Runnable[Input,Output]],)‚ÜíRunnableSerializable#",
    "configurable_alternatives",
    "(",
    "which:ConfigurableField",
    "*",
    "default_key:str='default'",
    "prefix_keys:bool=False",
    "**kwargs:Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]",
    ")",
    "‚ÜíRunnableSerializable",
    "‚Üí",
    "RunnableSerializable",
    "configurable_fields(**kwargs:ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption,)‚ÜíRunnableSerializable#",
    "configurable_fields",
    "(",
    "**kwargs:ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption",
    ")",
    "‚ÜíRunnableSerializable",
    "‚Üí",
    "RunnableSerializable",
    "get_num_tokens(text:str)‚Üíint#",
    "get_num_tokens",
    "(",
    "text:str",
    ")",
    "‚Üíint",
    "‚Üí",
    "int",
    "get_num_tokens_from_messages(messages:list[BaseMessage],tools:Sequence|None=None,)‚Üíint#",
    "get_num_tokens_from_messages",
    "(",
    "messages:list[BaseMessage]",
    "tools:Sequence|None=None",
    ")",
    "‚Üíint",
    "‚Üí",
    "int",
    "get_token_ids(text:str,)‚Üílist[int]#",
    "get_token_ids",
    "(",
    "text:str",
    ")",
    "‚Üílist[int]",
    "‚Üí",
    "list[int]",
    "invoke(input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚Üístr#",
    "invoke",
    "(",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚Üístr",
    "‚Üí",
    "str",
    "save(file_path:Path|str,)‚ÜíNone#",
    "save",
    "(",
    "file_path:Path|str",
    ")",
    "‚ÜíNone",
    "‚Üí",
    "None",
    "stream(input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚ÜíIterator[str]#",
    "stream",
    "(",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíIterator[str]",
    "‚Üí",
    "Iterator[str]",
    "with_alisteners(*,on_start:AsyncListener|None=None,on_end:AsyncListener|None=None,on_error:AsyncListener|None=None,)‚ÜíRunnable[Input,Output]#",
    "with_alisteners",
    "(",
    "*",
    "on_start:AsyncListener|None=None",
    "on_end:AsyncListener|None=None",
    "on_error:AsyncListener|None=None",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_config(config:RunnableConfig|None=None,**kwargs:Any,)‚ÜíRunnable[Input,Output]#",
    "with_config",
    "(",
    "config:RunnableConfig|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_fallbacks(fallbacks:Sequence[Runnable[Input,Output]],*,exceptions_to_handle:tuple[type[BaseException],...]=(<class'Exception'>,),exception_key:Optional[str]=None)‚ÜíRunnableWithFallbacksT[Input,Output]#",
    "with_fallbacks",
    "(",
    "fallbacks:Sequence[Runnable[Input,Output]],*,exceptions_to_handle:tuple[type[BaseException],...]=(<class'Exception'>,),exception_key:Optional[str]=None",
    ")",
    "‚ÜíRunnableWithFallbacksT[Input,Output]",
    "‚Üí",
    "RunnableWithFallbacksT[Input,Output]",
    "with_listeners(*,on_start:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None,on_end:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None,on_error:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None,)‚ÜíRunnable[Input,Output]#",
    "with_listeners",
    "(",
    "*",
    "on_start:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_end:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_error:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_retry(*,retry_if_exception_type:tuple[type[BaseException],...]=(<class'Exception'>,),wait_exponential_jitter:bool=True,exponential_jitter_params:Optional[ExponentialJitterParams]=None,stop_after_attempt:int=3)‚ÜíRunnable[Input,Output]#",
    "with_retry",
    "(",
    "*,retry_if_exception_type:tuple[type[BaseException],...]=(<class'Exception'>,),wait_exponential_jitter:bool=True,exponential_jitter_params:Optional[ExponentialJitterParams]=None,stop_after_attempt:int=3",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_structured_output(schema:dict|type,**kwargs:Any,)‚ÜíRunnable[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],dict|BaseModel]#",
    "with_structured_output",
    "(",
    "schema:dict|type",
    "**kwargs:Any",
    ")",
    "‚ÜíRunnable[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],dict|BaseModel]",
    "‚Üí",
    "Runnable[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],dict|BaseModel]",
    "with_types(*,input_type:type[Input]|None=None,output_type:type[Output]|None=None,)‚ÜíRunnable[Input,Output]#",
    "with_types",
    "(",
    "*",
    "input_type:type[Input]|None=None",
    "output_type:type[Output]|None=None",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]"
  ],
  "parameters": [
    "values:Dict",
    "prompt:str",
    "stop:list[str]|None=None",
    "callbacks:list[BaseCallbackHandler]|BaseCallbackManager|None=None",
    "*",
    "tags:list[str]|None=None",
    "metadata:dict[str,Any]|None=None",
    "**kwargs:Any",
    "inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "input:Any",
    "config:RunnableConfig|None=None",
    "*",
    "version:Literal['v1','v2']='v2'",
    "include_names:Sequence[str]|None=None",
    "include_types:Sequence[str]|None=None",
    "include_tags:Sequence[str]|None=None",
    "exclude_names:Sequence[str]|None=None",
    "exclude_types:Sequence[str]|None=None",
    "exclude_tags:Sequence[str]|None=None",
    "**kwargs:Any",
    "inputs:list[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    "**kwargs:Any",
    "which:ConfigurableField",
    "*",
    "default_key:str='default'",
    "prefix_keys:bool=False",
    "**kwargs:Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]",
    "**kwargs:ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption",
    "text:str",
    "messages:list[BaseMessage]",
    "tools:Sequence|None=None",
    "text:str",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "file_path:Path|str",
    "input:PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]]",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "*",
    "on_start:AsyncListener|None=None",
    "on_end:AsyncListener|None=None",
    "on_error:AsyncListener|None=None",
    "config:RunnableConfig|None=None",
    "**kwargs:Any",
    "fallbacks:Sequence[Runnable[Input,Output]],*,exceptions_to_handle:tuple[type[BaseException],...]=(<class'Exception'>,),exception_key:Optional[str]=None",
    "*",
    "on_start:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_end:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_error:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "*,retry_if_exception_type:tuple[type[BaseException],...]=(<class'Exception'>,),wait_exponential_jitter:bool=True,exponential_jitter_params:Optional[ExponentialJitterParams]=None,stop_after_attempt:int=3",
    "schema:dict|type",
    "**kwargs:Any",
    "*",
    "input_type:type[Input]|None=None",
    "output_type:type[Output]|None=None"
  ]
}
