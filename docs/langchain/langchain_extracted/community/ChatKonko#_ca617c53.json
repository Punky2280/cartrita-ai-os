{
  "url": "https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.konko.ChatKonko.html",
  "title": "ChatKonko#",
  "sections": [
    {
      "type": "li",
      "content": "LangChain Python API Reference"
    },
    {
      "type": "li",
      "content": "langchain-community: 0.3.29"
    },
    {
      "type": "li",
      "content": "chat_models"
    },
    {
      "type": "p",
      "content": "Bases:ChatOpenAI"
    },
    {
      "type": "p",
      "content": "ChatKonkoChat large language models API."
    },
    {
      "type": "p",
      "content": "To use, you should have thekonkopython package installed, and the\nenvironment variableKONKO_API_KEYandOPENAI_API_KEYset with your API key."
    },
    {
      "type": "p",
      "content": "Any parameters that are valid to be passed to the konko.create call can be passed\nin, even if not explicitly saved on this class."
    },
    {
      "type": "p",
      "content": "ChatKonko implements the standardRunnableInterface. üèÉ"
    },
    {
      "type": "p",
      "content": "TheRunnableInterfacehas additional methods that are available on runnables, such aswith_config,with_types,with_retry,assign,bind,get_graph, and more."
    },
    {
      "type": "p",
      "content": "Whether to cache the response."
    },
    {
      "type": "li",
      "content": "If true, will use the global cache."
    },
    {
      "type": "p",
      "content": "If true, will use the global cache."
    },
    {
      "type": "li",
      "content": "If false, will not use a cache"
    },
    {
      "type": "p",
      "content": "If false, will not use a cache"
    },
    {
      "type": "li",
      "content": "If None, will use the global cache if it‚Äôs set, otherwise no cache."
    },
    {
      "type": "p",
      "content": "If None, will use the global cache if it‚Äôs set, otherwise no cache."
    },
    {
      "type": "li",
      "content": "If instance ofBaseCache, will use the provided cache."
    },
    {
      "type": "p",
      "content": "If instance ofBaseCache, will use the provided cache."
    },
    {
      "type": "p",
      "content": "Caching is not currently supported for streaming methods of models."
    },
    {
      "type": "p",
      "content": "Deprecated since version 0.1.7:Usecallbacks()instead. It will be removed in pydantic==1.0."
    },
    {
      "type": "p",
      "content": "Callback manager to add to the run trace."
    },
    {
      "type": "p",
      "content": "Callbacks to add to the run trace."
    },
    {
      "type": "p",
      "content": "Optional encoder to use for counting tokens."
    },
    {
      "type": "p",
      "content": "Whether to disable streaming for this model."
    },
    {
      "type": "p",
      "content": "If streaming is bypassed, thenstream()/astream()/astream_events()will\ndefer toinvoke()/ainvoke()."
    },
    {
      "type": "li",
      "content": "If True, will always bypass streaming case."
    },
    {
      "type": "p",
      "content": "If True, will always bypass streaming case."
    },
    {
      "type": "li",
      "content": "If'tool_calling', will bypass streaming case only when the model is called\nwith atoolskeyword argument. In other words, LangChain will automatically\nswitch to non-streaming behavior (invoke()) only when the tools argument is\nprovided. This offers the best of both worlds."
    },
    {
      "type": "p",
      "content": "If'tool_calling', will bypass streaming case only when the model is called\nwith atoolskeyword argument. In other words, LangChain will automatically\nswitch to non-streaming behavior (invoke()) only when the tools argument is\nprovided. This offers the best of both worlds."
    },
    {
      "type": "li",
      "content": "If False (default), will always use streaming case if available."
    },
    {
      "type": "p",
      "content": "If False (default), will always use streaming case if available."
    },
    {
      "type": "p",
      "content": "The main reason for this flag is that code might be written usingstream()and\na user may want to swap out a given model for another model whose the implementation\ndoes not properly support streaming."
    },
    {
      "type": "p",
      "content": "Optional httpx.Client."
    },
    {
      "type": "p",
      "content": "Maximum number of retries to make when generating."
    },
    {
      "type": "p",
      "content": "Maximum number of tokens to generate."
    },
    {
      "type": "p",
      "content": "Metadata to add to the run trace."
    },
    {
      "type": "p",
      "content": "Model name to use."
    },
    {
      "type": "p",
      "content": "Holds any model parameters valid forcreatecall not explicitly specified."
    },
    {
      "type": "p",
      "content": "Model name to use."
    },
    {
      "type": "p",
      "content": "Number of chat completions to generate for each prompt."
    },
    {
      "type": "p",
      "content": "Base URL path for API requests, leave blank if not using a proxy or service\nemulator."
    },
    {
      "type": "p",
      "content": "Automatically inferred from env varOPENAI_API_KEYif not provided."
    },
    {
      "type": "p",
      "content": "Automatically inferred from env varOPENAI_ORG_IDif not provided."
    },
    {
      "type": "p",
      "content": "An optional rate limiter to use for limiting the number of requests."
    },
    {
      "type": "p",
      "content": "Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or\nNone."
    },
    {
      "type": "p",
      "content": "Whether to stream the results or not."
    },
    {
      "type": "p",
      "content": "Tags to add to the run trace."
    },
    {
      "type": "p",
      "content": "What sampling temperature to use."
    },
    {
      "type": "p",
      "content": "The model name to pass to tiktoken when using this class.\nTiktoken is used to count the number of tokens in documents to constrain\nthem to be under a certain limit. By default, when set to None, this will\nbe the same as the embedding model name. However, there are some cases\nwhere you may want to use this Embedding class with a model name not\nsupported by tiktoken. This can include when using Azure embeddings or\nwhen using one of the many model providers that expose an OpenAI-like\nAPI but with different models. In those cases, in order to avoid erroring\nwhen tiktoken is called, you can specify a model name to use here."
    },
    {
      "type": "p",
      "content": "Whether to print out response text."
    },
    {
      "type": "p",
      "content": "Validate that api key and python package exists in environment."
    },
    {
      "type": "p",
      "content": "values(Dict)"
    },
    {
      "type": "p",
      "content": "Get available models from Konko API."
    },
    {
      "type": "li",
      "content": "konko_api_key(str|SecretStr|None)"
    },
    {
      "type": "p",
      "content": "konko_api_key(str|SecretStr|None)"
    },
    {
      "type": "li",
      "content": "openai_api_key(str|SecretStr|None)"
    },
    {
      "type": "p",
      "content": "openai_api_key(str|SecretStr|None)"
    },
    {
      "type": "li",
      "content": "konko_api_base(str)"
    },
    {
      "type": "p",
      "content": "konko_api_base(str)"
    },
    {
      "type": "p",
      "content": "Deprecated since version 0.1.7:Useinvoke()instead. It will not be removed until langchain-core==1.0."
    },
    {
      "type": "p",
      "content": "Call the model."
    },
    {
      "type": "li",
      "content": "messages(list[BaseMessage]) ‚Äì List of messages."
    },
    {
      "type": "p",
      "content": "messages(list[BaseMessage]) ‚Äì List of messages."
    },
    {
      "type": "li",
      "content": "stop(list[str]|None) ‚Äì Stop words to use when generating. Model output is cut off at the\nfirst occurrence of any of these substrings."
    },
    {
      "type": "p",
      "content": "stop(list[str]|None) ‚Äì Stop words to use when generating. Model output is cut off at the\nfirst occurrence of any of these substrings."
    },
    {
      "type": "li",
      "content": "callbacks(list[BaseCallbackHandler]|BaseCallbackManager|None) ‚Äì Callbacks to pass through. Used for executing additional\nfunctionality, such as logging or streaming, throughout generation."
    },
    {
      "type": "p",
      "content": "callbacks(list[BaseCallbackHandler]|BaseCallbackManager|None) ‚Äì Callbacks to pass through. Used for executing additional\nfunctionality, such as logging or streaming, throughout generation."
    },
    {
      "type": "li",
      "content": "**kwargs(Any) ‚Äì Arbitrary additional keyword arguments. These are usually passed\nto the model provider API call."
    },
    {
      "type": "p",
      "content": "**kwargs(Any) ‚Äì Arbitrary additional keyword arguments. These are usually passed\nto the model provider API call."
    },
    {
      "type": "p",
      "content": "ValueError‚Äì If the generation is not a chat generation."
    },
    {
      "type": "p",
      "content": "The model output message."
    },
    {
      "type": "p",
      "content": "BaseMessage"
    },
    {
      "type": "p",
      "content": "Default implementation runsainvokein parallel usingasyncio.gather."
    },
    {
      "type": "p",
      "content": "The default implementation ofbatchworks well for IO bound runnables."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they can batch more efficiently;\ne.g., if the underlyingRunnableuses an API which supports a batch mode."
    },
    {
      "type": "li",
      "content": "inputs(list[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(list[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "**kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A list of outputs from theRunnable."
    },
    {
      "type": "p",
      "content": "list[Output]"
    },
    {
      "type": "p",
      "content": "Runainvokein parallel on a list of inputs."
    },
    {
      "type": "p",
      "content": "Yields results as they complete."
    },
    {
      "type": "li",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A tuple of the index of the input and the output from theRunnable."
    },
    {
      "type": "p",
      "content": "AsyncIterator[tuple[int,Output| Exception]]"
    },
    {
      "type": "p",
      "content": "Transform a single input into an output."
    },
    {
      "type": "li",
      "content": "input(LanguageModelInput) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(LanguageModelInput) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(Optional[RunnableConfig]) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(Optional[RunnableConfig]) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "stop(Optional[list[str]])"
    },
    {
      "type": "p",
      "content": "stop(Optional[list[str]])"
    },
    {
      "type": "li",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "BaseMessage"
    },
    {
      "type": "p",
      "content": "Default implementation ofastream, which callsainvoke."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they support streaming output."
    },
    {
      "type": "li",
      "content": "input(LanguageModelInput) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(LanguageModelInput) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(Optional[RunnableConfig]) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(Optional[RunnableConfig]) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "li",
      "content": "stop(Optional[list[str]])"
    },
    {
      "type": "p",
      "content": "stop(Optional[list[str]])"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "AsyncIterator[BaseMessageChunk]"
    },
    {
      "type": "p",
      "content": "Generate a stream of events."
    },
    {
      "type": "p",
      "content": "Use to create an iterator overStreamEventsthat provide real-time information\nabout the progress of theRunnable, includingStreamEventsfrom intermediate\nresults."
    },
    {
      "type": "p",
      "content": "AStreamEventis a dictionary with the following schema:"
    },
    {
      "type": "li",
      "content": "event:str- Event names are of the format:on_[runnable_type]_(start|stream|end)."
    },
    {
      "type": "p",
      "content": "event:str- Event names are of the format:on_[runnable_type]_(start|stream|end)."
    },
    {
      "type": "li",
      "content": "name:str- The name of theRunnablethat generated the event."
    },
    {
      "type": "p",
      "content": "name:str- The name of theRunnablethat generated the event."
    },
    {
      "type": "li",
      "content": "run_id:str- randomly generated ID associated with the given\nexecution of theRunnablethat emitted the event. A childRunnablethat gets\ninvoked as part of the execution of a parentRunnableis assigned its own\nunique ID."
    },
    {
      "type": "p",
      "content": "run_id:str- randomly generated ID associated with the given\nexecution of theRunnablethat emitted the event. A childRunnablethat gets\ninvoked as part of the execution of a parentRunnableis assigned its own\nunique ID."
    },
    {
      "type": "li",
      "content": "parent_ids:list[str]- The IDs of the parent runnables that generated\nthe event. The rootRunnablewill have an empty list. The order of the parent\nIDs is from the root to the immediate parent. Only available for v2 version of\nthe API. The v1 version of the API will return an empty list."
    },
    {
      "type": "p",
      "content": "parent_ids:list[str]- The IDs of the parent runnables that generated\nthe event. The rootRunnablewill have an empty list. The order of the parent\nIDs is from the root to the immediate parent. Only available for v2 version of\nthe API. The v1 version of the API will return an empty list."
    },
    {
      "type": "li",
      "content": "tags:Optional[list[str]]- The tags of theRunnablethat generated\nthe event."
    },
    {
      "type": "p",
      "content": "tags:Optional[list[str]]- The tags of theRunnablethat generated\nthe event."
    },
    {
      "type": "li",
      "content": "metadata:Optional[dict[str, Any]]- The metadata of theRunnablethat\ngenerated the event."
    },
    {
      "type": "p",
      "content": "metadata:Optional[dict[str, Any]]- The metadata of theRunnablethat\ngenerated the event."
    },
    {
      "type": "li",
      "content": "data:dict[str, Any]"
    },
    {
      "type": "p",
      "content": "data:dict[str, Any]"
    },
    {
      "type": "p",
      "content": "Below is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table."
    },
    {
      "type": "p",
      "content": "This reference table is for the v2 version of the schema."
    },
    {
      "type": "p",
      "content": "on_chat_model_start"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "{\"messages\":[[SystemMessage,HumanMessage]]}"
    },
    {
      "type": "p",
      "content": "on_chat_model_stream"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "AIMessageChunk(content=\"hello\")"
    },
    {
      "type": "p",
      "content": "on_chat_model_end"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "{\"messages\":[[SystemMessage,HumanMessage]]}"
    },
    {
      "type": "p",
      "content": "AIMessageChunk(content=\"helloworld\")"
    },
    {
      "type": "p",
      "content": "on_llm_start"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "{'input':'hello'}"
    },
    {
      "type": "p",
      "content": "on_llm_stream"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "``‚ÄôHello‚Äô ``"
    },
    {
      "type": "p",
      "content": "[model name]"
    },
    {
      "type": "p",
      "content": "'Hellohuman!'"
    },
    {
      "type": "p",
      "content": "on_chain_start"
    },
    {
      "type": "p",
      "content": "format_docs"
    },
    {
      "type": "p",
      "content": "on_chain_stream"
    },
    {
      "type": "p",
      "content": "format_docs"
    },
    {
      "type": "p",
      "content": "'helloworld!,goodbyeworld!'"
    },
    {
      "type": "p",
      "content": "on_chain_end"
    },
    {
      "type": "p",
      "content": "format_docs"
    },
    {
      "type": "p",
      "content": "[Document(...)]"
    },
    {
      "type": "p",
      "content": "'helloworld!,goodbyeworld!'"
    },
    {
      "type": "p",
      "content": "on_tool_start"
    },
    {
      "type": "p",
      "content": "{\"x\":1,\"y\":\"2\"}"
    },
    {
      "type": "p",
      "content": "on_tool_end"
    },
    {
      "type": "p",
      "content": "{\"x\":1,\"y\":\"2\"}"
    },
    {
      "type": "p",
      "content": "on_retriever_start"
    },
    {
      "type": "p",
      "content": "[retriever name]"
    },
    {
      "type": "p",
      "content": "{\"query\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "on_retriever_end"
    },
    {
      "type": "p",
      "content": "[retriever name]"
    },
    {
      "type": "p",
      "content": "{\"query\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "[Document(...),..]"
    },
    {
      "type": "p",
      "content": "on_prompt_start"
    },
    {
      "type": "p",
      "content": "[template_name]"
    },
    {
      "type": "p",
      "content": "{\"question\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "on_prompt_end"
    },
    {
      "type": "p",
      "content": "[template_name]"
    },
    {
      "type": "p",
      "content": "{\"question\":\"hello\"}"
    },
    {
      "type": "p",
      "content": "ChatPromptValue(messages:[SystemMessage,...])"
    },
    {
      "type": "p",
      "content": "In addition to the standard events, users can also dispatch custom events (see example below)."
    },
    {
      "type": "p",
      "content": "Custom events will be only be surfaced with in the v2 version of the API!"
    },
    {
      "type": "p",
      "content": "A custom event has following format:"
    },
    {
      "type": "p",
      "content": "Description"
    },
    {
      "type": "p",
      "content": "A user defined name for the event."
    },
    {
      "type": "p",
      "content": "The data associated with the event. This can be anything, though we suggest making it JSON serializable."
    },
    {
      "type": "p",
      "content": "Here are declarations associated with the standard events shown above:"
    },
    {
      "type": "p",
      "content": "format_docs:"
    },
    {
      "type": "p",
      "content": "Example: Dispatch Custom Event"
    },
    {
      "type": "li",
      "content": "input(Any) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(Any) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(Optional[RunnableConfig]) ‚Äì The config to use for theRunnable."
    },
    {
      "type": "p",
      "content": "config(Optional[RunnableConfig]) ‚Äì The config to use for theRunnable."
    },
    {
      "type": "li",
      "content": "version(Literal['v1','v2']) ‚Äì The version of the schema to use either'v2'or'v1'.\nUsers should use'v2'.'v1'is for backwards compatibility and will be deprecated\nin 0.4.0.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in'v2'."
    },
    {
      "type": "p",
      "content": "version(Literal['v1','v2']) ‚Äì The version of the schema to use either'v2'or'v1'.\nUsers should use'v2'.'v1'is for backwards compatibility and will be deprecated\nin 0.4.0.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in'v2'."
    },
    {
      "type": "li",
      "content": "include_names(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching names."
    },
    {
      "type": "p",
      "content": "include_names(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching names."
    },
    {
      "type": "li",
      "content": "include_types(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching types."
    },
    {
      "type": "p",
      "content": "include_types(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching types."
    },
    {
      "type": "li",
      "content": "include_tags(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching tags."
    },
    {
      "type": "p",
      "content": "include_tags(Optional[Sequence[str]]) ‚Äì Only include events fromRunnableswith matching tags."
    },
    {
      "type": "li",
      "content": "exclude_names(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching names."
    },
    {
      "type": "p",
      "content": "exclude_names(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching names."
    },
    {
      "type": "li",
      "content": "exclude_types(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching types."
    },
    {
      "type": "p",
      "content": "exclude_types(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching types."
    },
    {
      "type": "li",
      "content": "exclude_tags(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching tags."
    },
    {
      "type": "p",
      "content": "exclude_tags(Optional[Sequence[str]]) ‚Äì Exclude events fromRunnableswith matching tags."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable.\nThese will be passed toastream_logas this implementation\nofastream_eventsis built on top ofastream_log."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable.\nThese will be passed toastream_logas this implementation\nofastream_eventsis built on top ofastream_log."
    },
    {
      "type": "p",
      "content": "An async stream ofStreamEvents."
    },
    {
      "type": "p",
      "content": "NotImplementedError‚Äì If the version is not'v1'or'v2'."
    },
    {
      "type": "p",
      "content": "AsyncIterator[StreamEvent]"
    },
    {
      "type": "p",
      "content": "Default implementation runs invoke in parallel using a thread pool executor."
    },
    {
      "type": "p",
      "content": "The default implementation of batch works well for IO bound runnables."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they can batch more efficiently;\ne.g., if the underlyingRunnableuses an API which supports a batch mode."
    },
    {
      "type": "li",
      "content": "inputs(list[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(list[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work\nto do in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|list[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work\nto do in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "**kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A list of outputs from theRunnable."
    },
    {
      "type": "p",
      "content": "list[Output]"
    },
    {
      "type": "p",
      "content": "Runinvokein parallel on a list of inputs."
    },
    {
      "type": "p",
      "content": "Yields results as they complete."
    },
    {
      "type": "li",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "p",
      "content": "inputs(Sequence[Input]) ‚Äì A list of inputs to theRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|Sequence[RunnableConfig]|None) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "return_exceptions(bool) ‚Äì Whether to return exceptions instead of raising them.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "**kwargs(Any|None) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "Tuples of the index of the input and the output from theRunnable."
    },
    {
      "type": "p",
      "content": "Iterator[tuple[int,Output| Exception]]"
    },
    {
      "type": "p",
      "content": "Bind arguments to aRunnable, returning a newRunnable."
    },
    {
      "type": "p",
      "content": "Useful when aRunnablein a chain requires an argument that is not\nin the output of the previousRunnableor included in the user input."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì The arguments to bind to theRunnable."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the arguments bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input,Output]"
    },
    {
      "type": "p",
      "content": "Bind functions (and other objects) to this chat model."
    },
    {
      "type": "li",
      "content": "functions(Sequence[Dict[str,Any]|Type[BaseModel]|Callable]) ‚Äì A list of function definitions to bind to this chat model.\nCan be  a dictionary, pydantic model, or callable. Pydantic\nmodels and callables will be automatically converted to\ntheir schema dictionary representation."
    },
    {
      "type": "p",
      "content": "functions(Sequence[Dict[str,Any]|Type[BaseModel]|Callable]) ‚Äì A list of function definitions to bind to this chat model.\nCan be  a dictionary, pydantic model, or callable. Pydantic\nmodels and callables will be automatically converted to\ntheir schema dictionary representation."
    },
    {
      "type": "li",
      "content": "function_call(str|None) ‚Äì Which function to require the model to call.\nMust be the name of the single provided function or\n‚Äúauto‚Äù to automatically determine which function to call\n(if any)."
    },
    {
      "type": "p",
      "content": "function_call(str|None) ‚Äì Which function to require the model to call.\nMust be the name of the single provided function or\n‚Äúauto‚Äù to automatically determine which function to call\n(if any)."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Any additional parameters to pass to theRunnableconstructor."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Any additional parameters to pass to theRunnableconstructor."
    },
    {
      "type": "p",
      "content": "Runnable[PromptValue| str |Sequence[BaseMessage| list[str] | tuple[str, str] | str | dict[str,Any]],BaseMessage]"
    },
    {
      "type": "p",
      "content": "Bind tools to the model."
    },
    {
      "type": "li",
      "content": "tools(Sequence[Union[Dict[str,Any],type,Callable,BaseTool]]) ‚Äì Sequence of tools to bind to the model."
    },
    {
      "type": "p",
      "content": "tools(Sequence[Union[Dict[str,Any],type,Callable,BaseTool]]) ‚Äì Sequence of tools to bind to the model."
    },
    {
      "type": "li",
      "content": "tool_choice(Optional[Union[str]]) ‚Äì The tool to use. If ‚Äúany‚Äù then any tool can be used."
    },
    {
      "type": "p",
      "content": "tool_choice(Optional[Union[str]]) ‚Äì The tool to use. If ‚Äúany‚Äù then any tool can be used."
    },
    {
      "type": "li",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "A Runnable that returns a message."
    },
    {
      "type": "p",
      "content": "Runnable[LanguageModelInput,BaseMessage]"
    },
    {
      "type": "p",
      "content": "Use tenacity to retry the completion call."
    },
    {
      "type": "li",
      "content": "run_manager(CallbackManagerForLLMRun|None)"
    },
    {
      "type": "p",
      "content": "run_manager(CallbackManagerForLLMRun|None)"
    },
    {
      "type": "li",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "Configure alternatives forRunnablesthat can be set at runtime."
    },
    {
      "type": "li",
      "content": "which(ConfigurableField) ‚Äì TheConfigurableFieldinstance that will be used to select the\nalternative."
    },
    {
      "type": "p",
      "content": "which(ConfigurableField) ‚Äì TheConfigurableFieldinstance that will be used to select the\nalternative."
    },
    {
      "type": "li",
      "content": "default_key(str) ‚Äì The default key to use if no alternative is selected.\nDefaults to'default'."
    },
    {
      "type": "p",
      "content": "default_key(str) ‚Äì The default key to use if no alternative is selected.\nDefaults to'default'."
    },
    {
      "type": "li",
      "content": "prefix_keys(bool) ‚Äì Whether to prefix the keys with theConfigurableFieldid.\nDefaults to False."
    },
    {
      "type": "p",
      "content": "prefix_keys(bool) ‚Äì Whether to prefix the keys with theConfigurableFieldid.\nDefaults to False."
    },
    {
      "type": "li",
      "content": "**kwargs(Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]) ‚Äì A dictionary of keys toRunnableinstances or callables that\nreturnRunnableinstances."
    },
    {
      "type": "p",
      "content": "**kwargs(Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]) ‚Äì A dictionary of keys toRunnableinstances or callables that\nreturnRunnableinstances."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the alternatives configured."
    },
    {
      "type": "p",
      "content": "RunnableSerializable"
    },
    {
      "type": "p",
      "content": "Configure particularRunnablefields at runtime."
    },
    {
      "type": "p",
      "content": "**kwargs(ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption) ‚Äì A dictionary ofConfigurableFieldinstances to configure."
    },
    {
      "type": "p",
      "content": "ValueError‚Äì If a configuration key is not found in theRunnable."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the fields configured."
    },
    {
      "type": "p",
      "content": "RunnableSerializable"
    },
    {
      "type": "p",
      "content": "Get the number of tokens present in the text."
    },
    {
      "type": "p",
      "content": "Useful for checking if an input fits in a model‚Äôs context window."
    },
    {
      "type": "p",
      "content": "text(str) ‚Äì The string input to tokenize."
    },
    {
      "type": "p",
      "content": "The integer number of tokens in the text."
    },
    {
      "type": "p",
      "content": "Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package."
    },
    {
      "type": "p",
      "content": "Official documentation:openai/openai-cookbookmain/examples/How_to_format_inputs_to_ChatGPT_models.ipynb"
    },
    {
      "type": "li",
      "content": "messages(List[BaseMessage])"
    },
    {
      "type": "p",
      "content": "messages(List[BaseMessage])"
    },
    {
      "type": "li",
      "content": "tools(Sequence[Dict[str,Any]|Type|Callable|BaseTool]|None)"
    },
    {
      "type": "p",
      "content": "tools(Sequence[Dict[str,Any]|Type|Callable|BaseTool]|None)"
    },
    {
      "type": "p",
      "content": "Get the tokens present in the text with tiktoken package."
    },
    {
      "type": "p",
      "content": "Transform a single input into an output."
    },
    {
      "type": "li",
      "content": "input(LanguageModelInput) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(LanguageModelInput) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(Optional[RunnableConfig]) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(Optional[RunnableConfig]) ‚Äì A config to use when invoking theRunnable.\nThe config supports standard keys like'tags','metadata'for\ntracing purposes,'max_concurrency'for controlling how much work to\ndo in parallel, and other keys. Please refer to theRunnableConfigfor more details. Defaults to None."
    },
    {
      "type": "li",
      "content": "stop(Optional[list[str]])"
    },
    {
      "type": "p",
      "content": "stop(Optional[list[str]])"
    },
    {
      "type": "li",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "BaseMessage"
    },
    {
      "type": "p",
      "content": "Default implementation ofstream, which callsinvoke."
    },
    {
      "type": "p",
      "content": "Subclasses should override this method if they support streaming output."
    },
    {
      "type": "li",
      "content": "input(LanguageModelInput) ‚Äì The input to theRunnable."
    },
    {
      "type": "p",
      "content": "input(LanguageModelInput) ‚Äì The input to theRunnable."
    },
    {
      "type": "li",
      "content": "config(Optional[RunnableConfig]) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "config(Optional[RunnableConfig]) ‚Äì The config to use for theRunnable. Defaults to None."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "li",
      "content": "stop(Optional[list[str]])"
    },
    {
      "type": "p",
      "content": "stop(Optional[list[str]])"
    },
    {
      "type": "p",
      "content": "The output of theRunnable."
    },
    {
      "type": "p",
      "content": "Iterator[BaseMessageChunk]"
    },
    {
      "type": "p",
      "content": "Bind async lifecycle listeners to aRunnable."
    },
    {
      "type": "p",
      "content": "Returns a newRunnable."
    },
    {
      "type": "p",
      "content": "The Run object contains information about the run, including itsid,type,input,output,error,start_time,end_time, and\nany tags or metadata added to the run."
    },
    {
      "type": "li",
      "content": "on_start(Optional[AsyncListener]) ‚Äì Called asynchronously before theRunnablestarts running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_start(Optional[AsyncListener]) ‚Äì Called asynchronously before theRunnablestarts running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_end(Optional[AsyncListener]) ‚Äì Called asynchronously after theRunnablefinishes running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_end(Optional[AsyncListener]) ‚Äì Called asynchronously after theRunnablefinishes running,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_error(Optional[AsyncListener]) ‚Äì Called asynchronously if theRunnablethrows an error,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_error(Optional[AsyncListener]) ‚Äì Called asynchronously if theRunnablethrows an error,\nwith theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the listeners bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input, Output]"
    },
    {
      "type": "p",
      "content": "Bind config to aRunnable, returning a newRunnable."
    },
    {
      "type": "li",
      "content": "config(RunnableConfig|None) ‚Äì The config to bind to theRunnable."
    },
    {
      "type": "p",
      "content": "config(RunnableConfig|None) ‚Äì The config to bind to theRunnable."
    },
    {
      "type": "li",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "kwargs(Any) ‚Äì Additional keyword arguments to pass to theRunnable."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the config bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input,Output]"
    },
    {
      "type": "p",
      "content": "Add fallbacks to aRunnable, returning a newRunnable."
    },
    {
      "type": "p",
      "content": "The newRunnablewill try the originalRunnable, and then each fallback\nin order, upon failures."
    },
    {
      "type": "li",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "p",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "li",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle.\nDefaults to(Exception,)."
    },
    {
      "type": "p",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle.\nDefaults to(Exception,)."
    },
    {
      "type": "li",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input. Defaults to None."
    },
    {
      "type": "p",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input. Defaults to None."
    },
    {
      "type": "p",
      "content": "A newRunnablethat will try the originalRunnable, and then each\nfallback in order, upon failures."
    },
    {
      "type": "p",
      "content": "RunnableWithFallbacksT[Input, Output]"
    },
    {
      "type": "li",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "p",
      "content": "fallbacks(Sequence[Runnable[Input,Output]]) ‚Äì A sequence of runnables to try if the originalRunnablefails."
    },
    {
      "type": "li",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle."
    },
    {
      "type": "p",
      "content": "exceptions_to_handle(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to handle."
    },
    {
      "type": "li",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input."
    },
    {
      "type": "p",
      "content": "exception_key(Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\nto fallbacks as part of the input under the specified key.\nIf None, exceptions will not be passed to fallbacks.\nIf used, the baseRunnableand its fallbacks must accept a\ndictionary as input."
    },
    {
      "type": "p",
      "content": "A newRunnablethat will try the originalRunnable, and then each\nfallback in order, upon failures."
    },
    {
      "type": "p",
      "content": "RunnableWithFallbacksT[Input, Output]"
    },
    {
      "type": "p",
      "content": "Bind lifecycle listeners to aRunnable, returning a newRunnable."
    },
    {
      "type": "p",
      "content": "The Run object contains information about the run, including itsid,type,input,output,error,start_time,end_time, and\nany tags or metadata added to the run."
    },
    {
      "type": "li",
      "content": "on_start(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called before theRunnablestarts running, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_start(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called before theRunnablestarts running, with theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_end(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called after theRunnablefinishes running, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_end(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called after theRunnablefinishes running, with theRunobject. Defaults to None."
    },
    {
      "type": "li",
      "content": "on_error(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called if theRunnablethrows an error, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "on_error(Optional[Union[Callable[[Run],None],Callable[[Run,RunnableConfig],None]]]) ‚Äì Called if theRunnablethrows an error, with theRunobject. Defaults to None."
    },
    {
      "type": "p",
      "content": "A newRunnablewith the listeners bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input, Output]"
    },
    {
      "type": "p",
      "content": "Create a new Runnable that retries the original Runnable on exceptions."
    },
    {
      "type": "li",
      "content": "retry_if_exception_type(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to retry on.\nDefaults to (Exception,)."
    },
    {
      "type": "p",
      "content": "retry_if_exception_type(tuple[type[BaseException],...]) ‚Äì A tuple of exception types to retry on.\nDefaults to (Exception,)."
    },
    {
      "type": "li",
      "content": "wait_exponential_jitter(bool) ‚Äì Whether to add jitter to the wait\ntime between retries. Defaults to True."
    },
    {
      "type": "p",
      "content": "wait_exponential_jitter(bool) ‚Äì Whether to add jitter to the wait\ntime between retries. Defaults to True."
    },
    {
      "type": "li",
      "content": "stop_after_attempt(int) ‚Äì The maximum number of attempts to make before\ngiving up. Defaults to 3."
    },
    {
      "type": "p",
      "content": "stop_after_attempt(int) ‚Äì The maximum number of attempts to make before\ngiving up. Defaults to 3."
    },
    {
      "type": "li",
      "content": "exponential_jitter_params(Optional[ExponentialJitterParams]) ‚Äì Parameters fortenacity.wait_exponential_jitter. Namely:initial,max,exp_base, andjitter(all float values)."
    },
    {
      "type": "p",
      "content": "exponential_jitter_params(Optional[ExponentialJitterParams]) ‚Äì Parameters fortenacity.wait_exponential_jitter. Namely:initial,max,exp_base, andjitter(all float values)."
    },
    {
      "type": "p",
      "content": "A new Runnable that retries the original Runnable on exceptions."
    },
    {
      "type": "p",
      "content": "Runnable[Input, Output]"
    },
    {
      "type": "p",
      "content": "Model wrapper that returns outputs formatted to match the given schema."
    },
    {
      "type": "li",
      "content": "schema(Union[Dict,type]) ‚ÄìThe output schema. Can be passed in as:an OpenAI function/tool schema,a JSON Schema,a TypedDict class,or a Pydantic class.Ifschemais a Pydantic class then the model output will be a\nPydantic instance of that class, and the model-generated fields will be\nvalidated by the Pydantic class. Otherwise the model output will be a\ndict and will not be validated. Seelangchain_core.utils.function_calling.convert_to_openai_tool()for more on how to properly specify types and descriptions of\nschema fields when specifying a Pydantic or TypedDict class."
    },
    {
      "type": "p",
      "content": "schema(Union[Dict,type]) ‚ÄìThe output schema. Can be passed in as:an OpenAI function/tool schema,a JSON Schema,a TypedDict class,or a Pydantic class.Ifschemais a Pydantic class then the model output will be a\nPydantic instance of that class, and the model-generated fields will be\nvalidated by the Pydantic class. Otherwise the model output will be a\ndict and will not be validated. Seelangchain_core.utils.function_calling.convert_to_openai_tool()for more on how to properly specify types and descriptions of\nschema fields when specifying a Pydantic or TypedDict class."
    },
    {
      "type": "p",
      "content": "The output schema. Can be passed in as:"
    },
    {
      "type": "li",
      "content": "an OpenAI function/tool schema,"
    },
    {
      "type": "p",
      "content": "an OpenAI function/tool schema,"
    },
    {
      "type": "li",
      "content": "a JSON Schema,"
    },
    {
      "type": "p",
      "content": "a JSON Schema,"
    },
    {
      "type": "li",
      "content": "a TypedDict class,"
    },
    {
      "type": "p",
      "content": "a TypedDict class,"
    },
    {
      "type": "li",
      "content": "or a Pydantic class."
    },
    {
      "type": "p",
      "content": "or a Pydantic class."
    },
    {
      "type": "p",
      "content": "Ifschemais a Pydantic class then the model output will be a\nPydantic instance of that class, and the model-generated fields will be\nvalidated by the Pydantic class. Otherwise the model output will be a\ndict and will not be validated. Seelangchain_core.utils.function_calling.convert_to_openai_tool()for more on how to properly specify types and descriptions of\nschema fields when specifying a Pydantic or TypedDict class."
    },
    {
      "type": "li",
      "content": "include_raw(bool) ‚Äì If False then only the parsed structured output is returned. If\nan error occurs during model output parsing it will be raised. If True\nthen both the raw model response (a BaseMessage) and the parsed model\nresponse will be returned. If an error occurs during output parsing it\nwill be caught and returned as well. The final output is always a dict\nwith keys'raw','parsed', and'parsing_error'."
    },
    {
      "type": "p",
      "content": "include_raw(bool) ‚Äì If False then only the parsed structured output is returned. If\nan error occurs during model output parsing it will be raised. If True\nthen both the raw model response (a BaseMessage) and the parsed model\nresponse will be returned. If an error occurs during output parsing it\nwill be caught and returned as well. The final output is always a dict\nwith keys'raw','parsed', and'parsing_error'."
    },
    {
      "type": "li",
      "content": "kwargs(Any)"
    },
    {
      "type": "p",
      "content": "kwargs(Any)"
    },
    {
      "type": "li",
      "content": "ValueError‚Äì If there are any unsupportedkwargs."
    },
    {
      "type": "p",
      "content": "ValueError‚Äì If there are any unsupportedkwargs."
    },
    {
      "type": "li",
      "content": "NotImplementedError‚Äì If the model does not implementwith_structured_output()."
    },
    {
      "type": "p",
      "content": "NotImplementedError‚Äì If the model does not implementwith_structured_output()."
    },
    {
      "type": "p",
      "content": "A Runnable that takes same inputs as alangchain_core.language_models.chat.BaseChatModel.Ifinclude_rawis False andschemais a Pydantic class, Runnable outputs\nan instance ofschema(i.e., a Pydantic object).Otherwise, ifinclude_rawis False then Runnable outputs a dict.Ifinclude_rawis True, then Runnable outputs a dict with keys:'raw': BaseMessage'parsed': None if there was a parsing error, otherwise the type depends on theschemaas described above.'parsing_error': Optional[BaseException]"
    },
    {
      "type": "p",
      "content": "A Runnable that takes same inputs as alangchain_core.language_models.chat.BaseChatModel."
    },
    {
      "type": "p",
      "content": "Ifinclude_rawis False andschemais a Pydantic class, Runnable outputs\nan instance ofschema(i.e., a Pydantic object)."
    },
    {
      "type": "p",
      "content": "Otherwise, ifinclude_rawis False then Runnable outputs a dict."
    },
    {
      "type": "p",
      "content": "Ifinclude_rawis True, then Runnable outputs a dict with keys:"
    },
    {
      "type": "li",
      "content": "'raw': BaseMessage"
    },
    {
      "type": "p",
      "content": "'raw': BaseMessage"
    },
    {
      "type": "li",
      "content": "'parsed': None if there was a parsing error, otherwise the type depends on theschemaas described above."
    },
    {
      "type": "p",
      "content": "'parsed': None if there was a parsing error, otherwise the type depends on theschemaas described above."
    },
    {
      "type": "li",
      "content": "'parsing_error': Optional[BaseException]"
    },
    {
      "type": "p",
      "content": "'parsing_error': Optional[BaseException]"
    },
    {
      "type": "p",
      "content": "Runnable[LanguageModelInput, Union[Dict,BaseModel]]"
    },
    {
      "type": "p",
      "content": "Changed in version 0.2.26:Added support for TypedDict class."
    },
    {
      "type": "p",
      "content": "Bind input and output types to aRunnable, returning a newRunnable."
    },
    {
      "type": "li",
      "content": "input_type(type[Input]|None) ‚Äì The input type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "input_type(type[Input]|None) ‚Äì The input type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "li",
      "content": "output_type(type[Output]|None) ‚Äì The output type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "output_type(type[Output]|None) ‚Äì The output type to bind to theRunnable. Defaults to None."
    },
    {
      "type": "p",
      "content": "A new Runnable with the types bound."
    },
    {
      "type": "p",
      "content": "Runnable[Input,Output]"
    },
    {
      "type": "p",
      "content": "Examples using ChatKonko"
    },
    {
      "type": "li",
      "content": "ChatKonkocachecallback_managercallbackscustom_get_token_idsdefault_headersdefault_querydisable_streaminghttp_clientkonko_api_keymax_retriesmax_tokensmetadatamodelmodel_kwargsmodel_namenopenai_api_baseopenai_api_keyopenai_organizationopenai_proxyrate_limiterrequest_timeoutstreamingtagstemperaturetiktoken_model_nameverbosevalidate_environment()get_available_models()__call__()abatch()abatch_as_completed()ainvoke()astream()astream_events()batch()batch_as_completed()bind()bind_functions()bind_tools()completion_with_retry()configurable_alternatives()configurable_fields()get_num_tokens()get_num_tokens_from_messages()get_token_ids()invoke()stream()with_alisteners()with_config()with_fallbacks()with_listeners()with_retry()with_structured_output()with_types()"
    },
    {
      "type": "li",
      "content": "callback_manager"
    },
    {
      "type": "li",
      "content": "custom_get_token_ids"
    },
    {
      "type": "li",
      "content": "default_headers"
    },
    {
      "type": "li",
      "content": "default_query"
    },
    {
      "type": "li",
      "content": "disable_streaming"
    },
    {
      "type": "li",
      "content": "http_client"
    },
    {
      "type": "li",
      "content": "konko_api_key"
    },
    {
      "type": "li",
      "content": "max_retries"
    },
    {
      "type": "li",
      "content": "model_kwargs"
    },
    {
      "type": "li",
      "content": "openai_api_base"
    },
    {
      "type": "li",
      "content": "openai_api_key"
    },
    {
      "type": "li",
      "content": "openai_organization"
    },
    {
      "type": "li",
      "content": "openai_proxy"
    },
    {
      "type": "li",
      "content": "rate_limiter"
    },
    {
      "type": "li",
      "content": "request_timeout"
    },
    {
      "type": "li",
      "content": "temperature"
    },
    {
      "type": "li",
      "content": "tiktoken_model_name"
    },
    {
      "type": "li",
      "content": "validate_environment()"
    },
    {
      "type": "li",
      "content": "get_available_models()"
    },
    {
      "type": "li",
      "content": "abatch_as_completed()"
    },
    {
      "type": "li",
      "content": "astream_events()"
    },
    {
      "type": "li",
      "content": "batch_as_completed()"
    },
    {
      "type": "li",
      "content": "bind_functions()"
    },
    {
      "type": "li",
      "content": "bind_tools()"
    },
    {
      "type": "li",
      "content": "completion_with_retry()"
    },
    {
      "type": "li",
      "content": "configurable_alternatives()"
    },
    {
      "type": "li",
      "content": "configurable_fields()"
    },
    {
      "type": "li",
      "content": "get_num_tokens()"
    },
    {
      "type": "li",
      "content": "get_num_tokens_from_messages()"
    },
    {
      "type": "li",
      "content": "get_token_ids()"
    },
    {
      "type": "li",
      "content": "with_alisteners()"
    },
    {
      "type": "li",
      "content": "with_config()"
    },
    {
      "type": "li",
      "content": "with_fallbacks()"
    },
    {
      "type": "li",
      "content": "with_listeners()"
    },
    {
      "type": "li",
      "content": "with_retry()"
    },
    {
      "type": "li",
      "content": "with_structured_output()"
    },
    {
      "type": "li",
      "content": "with_types()"
    }
  ],
  "code_examples": [
    "chat_models",
    "ChatOpenAI",
    "konko",
    "KONKO_API_KEY",
    "OPENAI_API_KEY",
    "fromlangchain_community.chat_modelsimportChatKonkollm=ChatKonko(model=\"meta-llama/Llama-2-13b-chat-hf\")",
    "RunnableInterface",
    "RunnableInterface",
    "with_config",
    "with_types",
    "with_retry",
    "assign",
    "bind",
    "get_graph",
    "BaseCache",
    "callbacks()",
    "stream()",
    "astream()",
    "astream_events()",
    "invoke()",
    "ainvoke()",
    "'tool_calling'",
    "tools",
    "invoke()",
    "stream()",
    "invoke()",
    "ainvoke",
    "asyncio.gather",
    "batch",
    "Runnable",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "ainvoke",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "astream",
    "ainvoke",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "StreamEvents",
    "Runnable",
    "StreamEvents",
    "StreamEvent",
    "event",
    "on_[runnable_type]_(start|stream|end)",
    "name",
    "Runnable",
    "run_id",
    "Runnable",
    "Runnable",
    "Runnable",
    "parent_ids",
    "Runnable",
    "tags",
    "Runnable",
    "metadata",
    "Runnable",
    "data",
    "on_chat_model_start",
    "{\"messages\":[[SystemMessage,HumanMessage]]}",
    "on_chat_model_stream",
    "AIMessageChunk(content=\"hello\")",
    "on_chat_model_end",
    "{\"messages\":[[SystemMessage,HumanMessage]]}",
    "AIMessageChunk(content=\"helloworld\")",
    "on_llm_start",
    "{'input':'hello'}",
    "on_llm_stream",
    "on_llm_end",
    "'Hellohuman!'",
    "on_chain_start",
    "on_chain_stream",
    "'helloworld!,goodbyeworld!'",
    "on_chain_end",
    "[Document(...)]",
    "'helloworld!,goodbyeworld!'",
    "on_tool_start",
    "{\"x\":1,\"y\":\"2\"}",
    "on_tool_end",
    "{\"x\":1,\"y\":\"2\"}",
    "on_retriever_start",
    "{\"query\":\"hello\"}",
    "on_retriever_end",
    "{\"query\":\"hello\"}",
    "[Document(...),..]",
    "on_prompt_start",
    "{\"question\":\"hello\"}",
    "on_prompt_end",
    "{\"question\":\"hello\"}",
    "ChatPromptValue(messages:[SystemMessage,...])",
    "format_docs",
    "defformat_docs(docs:list[Document])->str:'''Format the docs.'''return\", \".join([doc.page_contentfordocindocs])format_docs=RunnableLambda(format_docs)",
    "some_tool",
    "@tooldefsome_tool(x:int,y:str)->dict:'''Some_tool.'''return{\"x\":x,\"y\":y}",
    "prompt",
    "template=ChatPromptTemplate.from_messages([(\"system\",\"You are Cat Agent 007\"),(\"human\",\"{question}\")]).with_config({\"run_name\":\"my_template\",\"tags\":[\"my_template\"]})",
    "fromlangchain_core.runnablesimportRunnableLambdaasyncdefreverse(s:str)->str:returns[::-1]chain=RunnableLambda(func=reverse)events=[eventasyncforeventinchain.astream_events(\"hello\",version=\"v2\")]# will produce the following events (run_id, and parent_ids# has been omitted for brevity):[{\"data\":{\"input\":\"hello\"},\"event\":\"on_chain_start\",\"metadata\":{},\"name\":\"reverse\",\"tags\":[],},{\"data\":{\"chunk\":\"olleh\"},\"event\":\"on_chain_stream\",\"metadata\":{},\"name\":\"reverse\",\"tags\":[],},{\"data\":{\"output\":\"olleh\"},\"event\":\"on_chain_end\",\"metadata\":{},\"name\":\"reverse\",\"tags\":[],},]",
    "fromlangchain_core.callbacks.managerimport(adispatch_custom_event,)fromlangchain_core.runnablesimportRunnableLambda,RunnableConfigimportasyncioasyncdefslow_thing(some_input:str,config:RunnableConfig)->str:\"\"\"Do something that takes a long time.\"\"\"awaitasyncio.sleep(1)# Placeholder for some slow operationawaitadispatch_custom_event(\"progress_event\",{\"message\":\"Finished step 1 of 3\"},config=config# Must be included for python < 3.10)awaitasyncio.sleep(1)# Placeholder for some slow operationawaitadispatch_custom_event(\"progress_event\",{\"message\":\"Finished step 2 of 3\"},config=config# Must be included for python < 3.10)awaitasyncio.sleep(1)# Placeholder for some slow operationreturn\"Done\"slow_thing=RunnableLambda(slow_thing)asyncforeventinslow_thing.astream_events(\"some_input\",version=\"v2\"):print(event)",
    "Runnable",
    "Runnable",
    "'v2'",
    "'v1'",
    "'v2'",
    "'v1'",
    "'v2'",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnables",
    "Runnable",
    "astream_log",
    "astream_events",
    "astream_log",
    "StreamEvents",
    "'v1'",
    "'v2'",
    "Runnable",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "invoke",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "fromlangchain_ollamaimportChatOllamafromlangchain_core.output_parsersimportStrOutputParserllm=ChatOllama(model=\"llama2\")# Without bind.chain=llm|StrOutputParser()chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")# Output is 'One two three four five.'# With bind.chain=llm.bind(stop=[\"three\"])|StrOutputParser()chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")# Output is 'One two'",
    "Runnable",
    "Runnables",
    "ConfigurableField",
    "'default'",
    "ConfigurableField",
    "Runnable",
    "Runnable",
    "Runnable",
    "fromlangchain_anthropicimportChatAnthropicfromlangchain_core.runnables.utilsimportConfigurableFieldfromlangchain_openaiimportChatOpenAImodel=ChatAnthropic(model_name=\"claude-3-7-sonnet-20250219\").configurable_alternatives(ConfigurableField(id=\"llm\"),default_key=\"anthropic\",openai=ChatOpenAI(),)# uses the default model ChatAnthropicprint(model.invoke(\"which organization created you?\").content)# uses ChatOpenAIprint(model.with_config(configurable={\"llm\":\"openai\"}).invoke(\"which organization created you?\").content)",
    "Runnable",
    "ConfigurableField",
    "Runnable",
    "Runnable",
    "fromlangchain_core.runnablesimportConfigurableFieldfromlangchain_openaiimportChatOpenAImodel=ChatOpenAI(max_tokens=20).configurable_fields(max_tokens=ConfigurableField(id=\"output_token_number\",name=\"Max tokens in the output\",description=\"The maximum number of tokens in the output\",))# max_tokens = 20print(\"max_tokens_20: \",model.invoke(\"tell me something about chess\").content)# max_tokens = 200print(\"max_tokens_200: \",model.with_config(configurable={\"output_token_number\":200}).invoke(\"tell me something about chess\").content,)",
    "Runnable",
    "Runnable",
    "'tags'",
    "'metadata'",
    "'max_concurrency'",
    "RunnableConfig",
    "Runnable",
    "stream",
    "invoke",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "id",
    "type",
    "input",
    "output",
    "error",
    "start_time",
    "end_time",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "fromlangchain_core.runnablesimportRunnableLambda,Runnablefromdatetimeimportdatetime,timezoneimporttimeimportasynciodefformat_t(timestamp:float)->str:returndatetime.fromtimestamp(timestamp,tz=timezone.utc).isoformat()asyncdeftest_runnable(time_to_sleep:int):print(f\"Runnable[{time_to_sleep}s]: starts at{format_t(time.time())}\")awaitasyncio.sleep(time_to_sleep)print(f\"Runnable[{time_to_sleep}s]: ends at{format_t(time.time())}\")asyncdeffn_start(run_obj:Runnable):print(f\"on start callback starts at{format_t(time.time())}\")awaitasyncio.sleep(3)print(f\"on start callback ends at{format_t(time.time())}\")asyncdeffn_end(run_obj:Runnable):print(f\"on end callback starts at{format_t(time.time())}\")awaitasyncio.sleep(2)print(f\"on end callback ends at{format_t(time.time())}\")runnable=RunnableLambda(test_runnable).with_alisteners(on_start=fn_start,on_end=fn_end)asyncdefconcurrent_runs():awaitasyncio.gather(runnable.ainvoke(2),runnable.ainvoke(3))asyncio.run(concurrent_runs())Result:onstartcallbackstartsat2025-03-01T07:05:22.875378+00:00onstartcallbackstartsat2025-03-01T07:05:22.875495+00:00onstartcallbackendsat2025-03-01T07:05:25.878862+00:00onstartcallbackendsat2025-03-01T07:05:25.878947+00:00Runnable[2s]:startsat2025-03-01T07:05:25.879392+00:00Runnable[3s]:startsat2025-03-01T07:05:25.879804+00:00Runnable[2s]:endsat2025-03-01T07:05:27.881998+00:00onendcallbackstartsat2025-03-01T07:05:27.882360+00:00Runnable[3s]:endsat2025-03-01T07:05:28.881737+00:00onendcallbackstartsat2025-03-01T07:05:28.882428+00:00onendcallbackendsat2025-03-01T07:05:29.883893+00:00onendcallbackendsat2025-03-01T07:05:30.884831+00:00",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "(Exception,)",
    "Runnable",
    "Runnable",
    "Runnable",
    "fromtypingimportIteratorfromlangchain_core.runnablesimportRunnableGeneratordef_generate_immediate_error(input:Iterator)->Iterator[str]:raiseValueError()yield\"\"def_generate(input:Iterator)->Iterator[str]:yield from\"foo bar\"runnable=RunnableGenerator(_generate_immediate_error).with_fallbacks([RunnableGenerator(_generate)])print(\"\".join(runnable.stream({})))# foo bar",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "id",
    "type",
    "input",
    "output",
    "error",
    "start_time",
    "end_time",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "Run",
    "Runnable",
    "fromlangchain_core.runnablesimportRunnableLambdafromlangchain_core.tracers.schemasimportRunimporttimedeftest_runnable(time_to_sleep:int):time.sleep(time_to_sleep)deffn_start(run_obj:Run):print(\"start_time:\",run_obj.start_time)deffn_end(run_obj:Run):print(\"end_time:\",run_obj.end_time)chain=RunnableLambda(test_runnable).with_listeners(on_start=fn_start,on_end=fn_end)chain.invoke(2)",
    "tenacity.wait_exponential_jitter",
    "initial",
    "max",
    "exp_base",
    "jitter",
    "fromlangchain_core.runnablesimportRunnableLambdacount=0def_lambda(x:int)->None:globalcountcount=count+1ifx==1:raiseValueError(\"x is 1\")else:passrunnable=RunnableLambda(_lambda)try:runnable.with_retry(stop_after_attempt=2,retry_if_exception_type=(ValueError,),).invoke(1)exceptValueError:passassertcount==2",
    "schema",
    "langchain_core.utils.function_calling.convert_to_openai_tool()",
    "'raw'",
    "'parsed'",
    "'parsing_error'",
    "kwargs",
    "with_structured_output()",
    "langchain_core.language_models.chat.BaseChatModel",
    "include_raw",
    "schema",
    "schema",
    "include_raw",
    "include_raw",
    "'raw'",
    "'parsed'",
    "schema",
    "'parsing_error'",
    "frompydanticimportBaseModelclassAnswerWithJustification(BaseModel):'''An answer to the user question along with justification for the answer.'''answer:strjustification:strllm=ChatModel(model=\"model-name\",temperature=0)structured_llm=llm.with_structured_output(AnswerWithJustification)structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")# -> AnswerWithJustification(#     answer='They weigh the same',#     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'# )",
    "frompydanticimportBaseModelclassAnswerWithJustification(BaseModel):'''An answer to the user question along with justification for the answer.'''answer:strjustification:strllm=ChatModel(model=\"model-name\",temperature=0)structured_llm=llm.with_structured_output(AnswerWithJustification,include_raw=True)structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")# -> {#     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),#     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),#     'parsing_error': None# }",
    "frompydanticimportBaseModelfromlangchain_core.utils.function_callingimportconvert_to_openai_toolclassAnswerWithJustification(BaseModel):'''An answer to the user question along with justification for the answer.'''answer:strjustification:strdict_schema=convert_to_openai_tool(AnswerWithJustification)llm=ChatModel(model=\"model-name\",temperature=0)structured_llm=llm.with_structured_output(dict_schema)structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")# -> {#     'answer': 'They weigh the same',#     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'# }",
    "Runnable",
    "Runnable",
    "Runnable",
    "Runnable",
    "ChatKonko",
    "cache",
    "callback_manager",
    "callbacks",
    "custom_get_token_ids",
    "default_headers",
    "default_query",
    "disable_streaming",
    "http_client",
    "konko_api_key",
    "max_retries",
    "max_tokens",
    "metadata",
    "model",
    "model_kwargs",
    "model_name",
    "n",
    "openai_api_base",
    "openai_api_key",
    "openai_organization",
    "openai_proxy",
    "rate_limiter",
    "request_timeout",
    "streaming",
    "tags",
    "temperature",
    "tiktoken_model_name",
    "verbose",
    "validate_environment()",
    "get_available_models()",
    "__call__()",
    "abatch()",
    "abatch_as_completed()",
    "ainvoke()",
    "astream()",
    "astream_events()",
    "batch()",
    "batch_as_completed()",
    "bind()",
    "bind_functions()",
    "bind_tools()",
    "completion_with_retry()",
    "configurable_alternatives()",
    "configurable_fields()",
    "get_num_tokens()",
    "get_num_tokens_from_messages()",
    "get_token_ids()",
    "invoke()",
    "stream()",
    "with_alisteners()",
    "with_config()",
    "with_fallbacks()",
    "with_listeners()",
    "with_retry()",
    "with_structured_output()",
    "with_types()"
  ],
  "api_signatures": [
    "classlangchain_community.chat_models.konko.ChatKonko[source]#",
    "langchain_community.chat_models.konko.",
    "ChatKonko",
    "paramcache:BaseCache|bool|None=None#",
    "cache",
    "paramcallback_manager:BaseCallbackManager|None=None#",
    "callback_manager",
    "paramcallbacks:Callbacks=None#",
    "callbacks",
    "paramcustom_get_token_ids:Callable[[str],list[int]]|None=None#",
    "custom_get_token_ids",
    "paramdefault_headers:Mapping[str,str]|None=None#",
    "default_headers",
    "paramdefault_query:Mapping[str,object]|None=None#",
    "default_query",
    "paramdisable_streaming:bool|Literal['tool_calling']=False#",
    "disable_streaming",
    "paramhttp_client:Any|None=None#",
    "http_client",
    "paramkonko_api_key:str|None=None#",
    "konko_api_key",
    "parammax_retries:int=6#",
    "max_retries",
    "parammax_tokens:int=20#",
    "max_tokens",
    "parammetadata:dict[str,Any]|None=None#",
    "metadata",
    "parammodel:str='meta-llama/Llama-2-13b-chat-hf'#",
    "model",
    "parammodel_kwargs:Dict[str,Any][Optional]#",
    "model_kwargs",
    "parammodel_name:str='gpt-3.5-turbo'(alias'model')#",
    "model_name",
    "paramn:int=1#",
    "n",
    "paramopenai_api_base:str|None=None(alias'base_url')#",
    "openai_api_base",
    "paramopenai_api_key:str|None=None#",
    "openai_api_key",
    "paramopenai_organization:str|None=None(alias'organization')#",
    "openai_organization",
    "paramopenai_proxy:str|None=None#",
    "openai_proxy",
    "paramrate_limiter:BaseRateLimiter|None=None#",
    "rate_limiter",
    "paramrequest_timeout:float|Tuple[float,float]|Any|None=None(alias'timeout')#",
    "request_timeout",
    "paramstreaming:bool=False#",
    "streaming",
    "paramtags:list[str]|None=None#",
    "tags",
    "paramtemperature:float=0.7#",
    "temperature",
    "paramtiktoken_model_name:str|None=None#",
    "tiktoken_model_name",
    "paramverbose:bool[Optional]#",
    "verbose",
    "classmethodvalidate_environment(values:Dict,)‚ÜíDict[source]#",
    "validate_environment",
    "(",
    "values:Dict",
    ")",
    "‚ÜíDict",
    "‚Üí",
    "Dict",
    "staticget_available_models(konko_api_key:str|SecretStr|None=None,openai_api_key:str|SecretStr|None=None,konko_api_base:str='https://api.konko.ai/v1',)‚ÜíSet[str][source]#",
    "get_available_models",
    "(",
    "konko_api_key:str|SecretStr|None=None",
    "openai_api_key:str|SecretStr|None=None",
    "konko_api_base:str='https://api.konko.ai/v1'",
    ")",
    "‚ÜíSet[str]",
    "‚Üí",
    "Set[str]",
    "__call__(messages:list[BaseMessage],stop:list[str]|None=None,callbacks:list[BaseCallbackHandler]|BaseCallbackManager|None=None,**kwargs:Any,)‚ÜíBaseMessage#",
    "__call__",
    "(",
    "messages:list[BaseMessage]",
    "stop:list[str]|None=None",
    "callbacks:list[BaseCallbackHandler]|BaseCallbackManager|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíBaseMessage",
    "‚Üí",
    "BaseMessage",
    "asyncabatch(inputs:list[Input],config:RunnableConfig|list[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any|None,)‚Üílist[Output]#",
    "abatch",
    "(",
    "inputs:list[Input]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    ")",
    "‚Üílist[Output]",
    "‚Üí",
    "list[Output]",
    "asyncabatch_as_completed(inputs:Sequence[Input],config:RunnableConfig|Sequence[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any|None,)‚ÜíAsyncIterator[tuple[int,Output|Exception]]#",
    "abatch_as_completed",
    "(",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    ")",
    "‚ÜíAsyncIterator[tuple[int,Output|Exception]]",
    "‚Üí",
    "AsyncIterator[tuple[int,Output|Exception]]",
    "asyncainvoke(input:LanguageModelInput,config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚ÜíBaseMessage#",
    "ainvoke",
    "(",
    "input:LanguageModelInput",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíBaseMessage",
    "‚Üí",
    "BaseMessage",
    "asyncastream(input:LanguageModelInput,config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚ÜíAsyncIterator[BaseMessageChunk]#",
    "astream",
    "(",
    "input:LanguageModelInput",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíAsyncIterator[BaseMessageChunk]",
    "‚Üí",
    "AsyncIterator[BaseMessageChunk]",
    "asyncastream_events(input:Any,config:RunnableConfig|None=None,*,version:Literal['v1','v2']='v2',include_names:Sequence[str]|None=None,include_types:Sequence[str]|None=None,include_tags:Sequence[str]|None=None,exclude_names:Sequence[str]|None=None,exclude_types:Sequence[str]|None=None,exclude_tags:Sequence[str]|None=None,**kwargs:Any,)‚ÜíAsyncIterator[StreamEvent]#",
    "astream_events",
    "(",
    "input:Any",
    "config:RunnableConfig|None=None",
    "*",
    "version:Literal['v1','v2']='v2'",
    "include_names:Sequence[str]|None=None",
    "include_types:Sequence[str]|None=None",
    "include_tags:Sequence[str]|None=None",
    "exclude_names:Sequence[str]|None=None",
    "exclude_types:Sequence[str]|None=None",
    "exclude_tags:Sequence[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíAsyncIterator[StreamEvent]",
    "‚Üí",
    "AsyncIterator[StreamEvent]",
    "batch(inputs:list[Input],config:RunnableConfig|list[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any|None,)‚Üílist[Output]#",
    "batch",
    "(",
    "inputs:list[Input]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    ")",
    "‚Üílist[Output]",
    "‚Üí",
    "list[Output]",
    "batch_as_completed(inputs:Sequence[Input],config:RunnableConfig|Sequence[RunnableConfig]|None=None,*,return_exceptions:bool=False,**kwargs:Any|None,)‚ÜíIterator[tuple[int,Output|Exception]]#",
    "batch_as_completed",
    "(",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    ")",
    "‚ÜíIterator[tuple[int,Output|Exception]]",
    "‚Üí",
    "Iterator[tuple[int,Output|Exception]]",
    "bind(**kwargs:Any,)‚ÜíRunnable[Input,Output]#",
    "bind",
    "(",
    "**kwargs:Any",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "bind_functions(functions:Sequence[Dict[str,Any]|Type[BaseModel]|Callable],function_call:str|None=None,**kwargs:Any,)‚ÜíRunnable[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],BaseMessage]#",
    "bind_functions",
    "(",
    "functions:Sequence[Dict[str,Any]|Type[BaseModel]|Callable]",
    "function_call:str|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíRunnable[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],BaseMessage]",
    "‚Üí",
    "Runnable[PromptValue|str|Sequence[BaseMessage|list[str]|tuple[str,str]|str|dict[str,Any]],BaseMessage]",
    "bind_tools(tools:Sequence[Dict[str,Any]|type|Callable|BaseTool],*,tool_choice:str|None=None,**kwargs:Any,)‚ÜíRunnable[LanguageModelInput,BaseMessage]#",
    "bind_tools",
    "(",
    "tools:Sequence[Dict[str,Any]|type|Callable|BaseTool]",
    "*",
    "tool_choice:str|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíRunnable[LanguageModelInput,BaseMessage]",
    "‚Üí",
    "Runnable[LanguageModelInput,BaseMessage]",
    "completion_with_retry(run_manager:CallbackManagerForLLMRun|None=None,**kwargs:Any,)‚ÜíAny[source]#",
    "completion_with_retry",
    "(",
    "run_manager:CallbackManagerForLLMRun|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíAny",
    "‚Üí",
    "Any",
    "configurable_alternatives(which:ConfigurableField,*,default_key:str='default',prefix_keys:bool=False,**kwargs:Runnable[Input,Output]|Callable[[],Runnable[Input,Output]],)‚ÜíRunnableSerializable#",
    "configurable_alternatives",
    "(",
    "which:ConfigurableField",
    "*",
    "default_key:str='default'",
    "prefix_keys:bool=False",
    "**kwargs:Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]",
    ")",
    "‚ÜíRunnableSerializable",
    "‚Üí",
    "RunnableSerializable",
    "configurable_fields(**kwargs:ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption,)‚ÜíRunnableSerializable#",
    "configurable_fields",
    "(",
    "**kwargs:ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption",
    ")",
    "‚ÜíRunnableSerializable",
    "‚Üí",
    "RunnableSerializable",
    "get_num_tokens(text:str)‚Üíint#",
    "get_num_tokens",
    "(",
    "text:str",
    ")",
    "‚Üíint",
    "‚Üí",
    "int",
    "get_num_tokens_from_messages(messages:List[BaseMessage],tools:Sequence[Dict[str,Any]|Type|Callable|BaseTool]|None=None,)‚Üíint#",
    "get_num_tokens_from_messages",
    "(",
    "messages:List[BaseMessage]",
    "tools:Sequence[Dict[str,Any]|Type|Callable|BaseTool]|None=None",
    ")",
    "‚Üíint",
    "‚Üí",
    "int",
    "get_token_ids(text:str)‚ÜíList[int]#",
    "get_token_ids",
    "(",
    "text:str",
    ")",
    "‚ÜíList[int]",
    "‚Üí",
    "List[int]",
    "invoke(input:LanguageModelInput,config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚ÜíBaseMessage#",
    "invoke",
    "(",
    "input:LanguageModelInput",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíBaseMessage",
    "‚Üí",
    "BaseMessage",
    "stream(input:LanguageModelInput,config:RunnableConfig|None=None,*,stop:list[str]|None=None,**kwargs:Any,)‚ÜíIterator[BaseMessageChunk]#",
    "stream",
    "(",
    "input:LanguageModelInput",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíIterator[BaseMessageChunk]",
    "‚Üí",
    "Iterator[BaseMessageChunk]",
    "with_alisteners(*,on_start:AsyncListener|None=None,on_end:AsyncListener|None=None,on_error:AsyncListener|None=None,)‚ÜíRunnable[Input,Output]#",
    "with_alisteners",
    "(",
    "*",
    "on_start:AsyncListener|None=None",
    "on_end:AsyncListener|None=None",
    "on_error:AsyncListener|None=None",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_config(config:RunnableConfig|None=None,**kwargs:Any,)‚ÜíRunnable[Input,Output]#",
    "with_config",
    "(",
    "config:RunnableConfig|None=None",
    "**kwargs:Any",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_fallbacks(fallbacks:Sequence[Runnable[Input,Output]],*,exceptions_to_handle:tuple[type[BaseException],...]=(<class'Exception'>,),exception_key:Optional[str]=None)‚ÜíRunnableWithFallbacksT[Input,Output]#",
    "with_fallbacks",
    "(",
    "fallbacks:Sequence[Runnable[Input,Output]],*,exceptions_to_handle:tuple[type[BaseException],...]=(<class'Exception'>,),exception_key:Optional[str]=None",
    ")",
    "‚ÜíRunnableWithFallbacksT[Input,Output]",
    "‚Üí",
    "RunnableWithFallbacksT[Input,Output]",
    "with_listeners(*,on_start:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None,on_end:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None,on_error:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None,)‚ÜíRunnable[Input,Output]#",
    "with_listeners",
    "(",
    "*",
    "on_start:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_end:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_error:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_retry(*,retry_if_exception_type:tuple[type[BaseException],...]=(<class'Exception'>,),wait_exponential_jitter:bool=True,exponential_jitter_params:Optional[ExponentialJitterParams]=None,stop_after_attempt:int=3)‚ÜíRunnable[Input,Output]#",
    "with_retry",
    "(",
    "*,retry_if_exception_type:tuple[type[BaseException],...]=(<class'Exception'>,),wait_exponential_jitter:bool=True,exponential_jitter_params:Optional[ExponentialJitterParams]=None,stop_after_attempt:int=3",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]",
    "with_structured_output(schema:Dict|type,*,include_raw:bool=False,**kwargs:Any,)‚ÜíRunnable[LanguageModelInput,Dict|BaseModel]#",
    "with_structured_output",
    "(",
    "schema:Dict|type",
    "*",
    "include_raw:bool=False",
    "**kwargs:Any",
    ")",
    "‚ÜíRunnable[LanguageModelInput,Dict|BaseModel]",
    "‚Üí",
    "Runnable[LanguageModelInput,Dict|BaseModel]",
    "with_types(*,input_type:type[Input]|None=None,output_type:type[Output]|None=None,)‚ÜíRunnable[Input,Output]#",
    "with_types",
    "(",
    "*",
    "input_type:type[Input]|None=None",
    "output_type:type[Output]|None=None",
    ")",
    "‚ÜíRunnable[Input,Output]",
    "‚Üí",
    "Runnable[Input,Output]"
  ],
  "parameters": [
    "values:Dict",
    "konko_api_key:str|SecretStr|None=None",
    "openai_api_key:str|SecretStr|None=None",
    "konko_api_base:str='https://api.konko.ai/v1'",
    "messages:list[BaseMessage]",
    "stop:list[str]|None=None",
    "callbacks:list[BaseCallbackHandler]|BaseCallbackManager|None=None",
    "**kwargs:Any",
    "inputs:list[Input]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    "input:LanguageModelInput",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "input:LanguageModelInput",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "input:Any",
    "config:RunnableConfig|None=None",
    "*",
    "version:Literal['v1','v2']='v2'",
    "include_names:Sequence[str]|None=None",
    "include_types:Sequence[str]|None=None",
    "include_tags:Sequence[str]|None=None",
    "exclude_names:Sequence[str]|None=None",
    "exclude_types:Sequence[str]|None=None",
    "exclude_tags:Sequence[str]|None=None",
    "**kwargs:Any",
    "inputs:list[Input]",
    "config:RunnableConfig|list[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    "inputs:Sequence[Input]",
    "config:RunnableConfig|Sequence[RunnableConfig]|None=None",
    "*",
    "return_exceptions:bool=False",
    "**kwargs:Any|None",
    "**kwargs:Any",
    "functions:Sequence[Dict[str,Any]|Type[BaseModel]|Callable]",
    "function_call:str|None=None",
    "**kwargs:Any",
    "tools:Sequence[Dict[str,Any]|type|Callable|BaseTool]",
    "*",
    "tool_choice:str|None=None",
    "**kwargs:Any",
    "run_manager:CallbackManagerForLLMRun|None=None",
    "**kwargs:Any",
    "which:ConfigurableField",
    "*",
    "default_key:str='default'",
    "prefix_keys:bool=False",
    "**kwargs:Runnable[Input,Output]|Callable[[],Runnable[Input,Output]]",
    "**kwargs:ConfigurableField|ConfigurableFieldSingleOption|ConfigurableFieldMultiOption",
    "text:str",
    "messages:List[BaseMessage]",
    "tools:Sequence[Dict[str,Any]|Type|Callable|BaseTool]|None=None",
    "text:str",
    "input:LanguageModelInput",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "input:LanguageModelInput",
    "config:RunnableConfig|None=None",
    "*",
    "stop:list[str]|None=None",
    "**kwargs:Any",
    "*",
    "on_start:AsyncListener|None=None",
    "on_end:AsyncListener|None=None",
    "on_error:AsyncListener|None=None",
    "config:RunnableConfig|None=None",
    "**kwargs:Any",
    "fallbacks:Sequence[Runnable[Input,Output]],*,exceptions_to_handle:tuple[type[BaseException],...]=(<class'Exception'>,),exception_key:Optional[str]=None",
    "*",
    "on_start:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_end:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "on_error:Callable[[Run],None]|Callable[[Run,RunnableConfig],None]|None=None",
    "*,retry_if_exception_type:tuple[type[BaseException],...]=(<class'Exception'>,),wait_exponential_jitter:bool=True,exponential_jitter_params:Optional[ExponentialJitterParams]=None,stop_after_attempt:int=3",
    "schema:Dict|type",
    "*",
    "include_raw:bool=False",
    "**kwargs:Any",
    "*",
    "input_type:type[Input]|None=None",
    "output_type:type[Output]|None=None"
  ]
}
